
### scVI: single-cell variational inference  {#ch-5-4-1}
scVI [@RN88]<!--[13]--> is designed to address a range of fundamental analysis tasks, including batch correction, visualization, clustering, and differential expression. 

*Model.* scVI  is a VAE that models the counts of each cell from different batches. scVI adopts a ZINB distribution for $x_{gn}$

\begin{equation}
p(x_{gn}\vert \pi_{gn},L_{n},\nu_{gn},\alpha) = \pi_{gn}\delta(0)+(1-\pi_{gn})NB(L_{n}\nu_{gn},\alpha_{g}) (\#eq:eq31)
\end{equation}

which is defined similarly as Eq (\@ref(eq:eq11)) in DCA, except that $L_{n}$ denotes the scaling factor for cell $n$, which follows a log-Normal ($\log \mathcal{N}$) prior as $p(L_{n}) = \log{\mathcal{N}(\mu_{L_{n}},\sigma_{L_{n}}^{2})}$, therefore, $v_{gn}$ represents the mean counts normalized by $L_{n}$. Now, let $s_n \in \{0,1\}^{B}$ be the batch ID of cell $n$ with $B$ being the total number of batches. Then, $ν_{gn}$ and $\pi_{g}$ are further modeled as functions of the d-dimension latent variable $z_{n} \in \mathbb{R}^{d}$ and the batch ID $s_{n}$ by the decoder networks $D_{θ_{ν}}$ and $D_{θ_{\pi}}$ as

\begin{equation}
\mathbf{ν}_{n}=D_{θ_{ν}}(z_{n},s_{n}), \boldsymbol{\pi}_{n} = D_{θ_{π}}(\mathbf{z}_{n},s_{n}) (\#eq:eq32)
\end{equation}

where the $g$th element of $\mathbf{ν}_{n}$ and $\boldsymbol{\pi}_{n}$ are $ν_{gn}$  and $\pi_{g}$, respectively, and $\boldsymbol{\theta}_{ν}$ and $\boldsymbol{\theta}_{\pi}$ are the decoder weights. Note that the lower layers of the two decoders are shared. For inference, both $z_{n}$ and $L_{n}$ are considered as latent variables and therefore $q(x_{n},s_{n}) = q(z_{n} | x_{n},s_{n})q(L_{n} | x_{n},s_{n})$  is a mean-field approximate to the intractable posterior distribution $p(z_{n},L_{n} | x_{n},s_{n})$ and

\begin{equation}
q(\mathbf{z}_{n}│x_{n},s_{n})= \mathcal{N}(\boldsymbol{\mu}_{z_{n}}, diag(\sigma_{Z_{n}}^{2})), \space  
q(L_{n}│x_{n},s_{n} ) =  \log\mathcal{N}(\mu_{L_{n}}, diag(\sigma_{L_{n}}^{2} )) (\#eq:eq33)
\end{equation}

whose means and variances $\{\mu_{z_{n}}, \sigma_{Z_{n}}^{2}\}$ and $\{\mu_{L_{n}},\sigma_{L_{n}}^{2} \}$ are defined by the encoder networks $E_{Z}$ and $E_{L}$ applied to $x_{n}$ and $s_{n}$ as

\begin{equation}
\{ \mu_{z_{n}},\sigma_{Z_{n}}^{2} \} = E_{\phi_{z}}(x_{n},s_{n}), \space \{\mu_{L_{n}},\sigma_{L_{n}}^{2} \} = E_{\phi_{L}}(z_{n},s_{n}) (\#eq:eq34)
\end{equation}

where  $\phi_{z}$, and $\phi_{L}$ are the encoder weights. Note that, like the decoders, the lower layers of the two encoders are also shared. Overall, the model parameters to be estimated by the variational optimization is $\Theta=\{ \theta_{ν}, \theta_{\pi}, \phi_{z}, \phi_{L}, \alpha_{g} \}$. After inference is made, the latent vectors $z_{n}$ are used for visualization and clustering. $ν_{gn}$ provides a batch-corrected, size-factor normalized estimate of gene expression for each gene $g$ in each cell $n$.  An added advantage of the probabilistic representation by scVI is that it provides a natural probabilistic treatment of the subsequent differential analysis, resulting in lower variance in the adopted hypothesis tests. 

*Evaluation metrics:* Run-time vs. dataset size was used to assess the scalability. To assess imputation, randomly selected non-zero entries in the count matrix were used as hold-out data and the $L_{1}$ distance between the imputed and the original values in the hold-out data were used to measure the imputation accuracy. For clustering, ARI, NMI, and the Silhouette index were used. The entropy of bach mixing was adopted to measure batch correction performance.

*Results:* scVI was evaluated for its scalability, the performance of imputation. For scalability, ScVI was shown to be faster than most nonDL algorithms and scalable to handle twice as many cells as nonDL algorithms with a fixed memory. For imputation, ScVI, together with other ZINB-based models, performed better than methods using alternative distributions. However, it underperformed for the dataset (HEMATO) with fewer cells. For the latent space, scVI was shown to provide a comparable stratification of cells into previously annotated cell types. Although scVI failed to ravel SIMLR, it is among the best in capturing biological structures (hierarchical structure, dynamics, etc.) and recognizing noise in data. For batch correction, it outperforms ComBat. For normalizing sequencing depth, the size factor inferred by scVI was shown to be strongly correlated with the sequencing depth. Interestingly, the negative binomial distribution in the ZINB was found to explain the proportions of zero expressions in the cells, whereas the zero probability $\pi_{gn}$ is found to be more correlated with alignment errors. For differential expression analysis, scVI was shown to be among the best.    

