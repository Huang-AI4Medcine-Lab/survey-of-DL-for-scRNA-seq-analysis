<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Overview of common deep learning models for scRNA-seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis</title>
  <meta name="description" content="3 Overview of common deep learning models for scRNA-seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Overview of common deep learning models for scRNA-seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Huang-AI4Medicine-Lab/survey-of-DL-for-scRNA-seq-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Overview of common deep learning models for scRNA-seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  
  
  

<meta name="author" content="Mario Flores1§, Zhentao Liu1, Tinghe Zhang1, Md Musaddaqui Hasib1, Yu-Chiao Chiu2, Zhenqing Ye2,3, Karla Paniagua1, Sumin Jo1, Jianqiu Zhang1, Shou-Jiang Gao4,6, Yufang Jin1, Yidong Chen2,3§, and Yufei Huang5,6§" />


<meta name="date" content="2021-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-2.html"/>
<link rel="next" href="ch-4.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Survey of Deep Learning for scRNA-seq Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the authors</a></li>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html"><i class="fa fa-check"></i>About this book</a>
<ul>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html#key-points"><i class="fa fa-check"></i>Key Points</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-1.html"><a href="ch-1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="ch-2.html"><a href="ch-2.html"><i class="fa fa-check"></i><b>2</b> Overview of scRNA-seq processing pipeline</a></li>
<li class="chapter" data-level="3" data-path="ch-3.html"><a href="ch-3.html"><i class="fa fa-check"></i><b>3</b> Overview of common deep learning models for scRNA-seq analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-3.html"><a href="ch-3.html#ch-3-1"><i class="fa fa-check"></i><b>3.1</b> Variational Autoencoder (VAEs) for scRNA-Seq data</a></li>
<li class="chapter" data-level="3.2" data-path="ch-3.html"><a href="ch-3.html#ch-3-2"><i class="fa fa-check"></i><b>3.2</b> Autoencoders (AEs) for scRNA-seq data</a></li>
<li class="chapter" data-level="3.3" data-path="ch-3.html"><a href="ch-3.html#ch-3-3"><i class="fa fa-check"></i><b>3.3</b> Generative adversarial networks (GANs) for scRNA-seq data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-4.html"><a href="ch-4.html"><i class="fa fa-check"></i><b>4</b> Overview of datasets and evaluation metrics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-4.html"><a href="ch-4.html#ch-4-1"><i class="fa fa-check"></i><b>4.1</b> Evaluation methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-1"><i class="fa fa-check"></i><b>4.1.1</b> Imputation</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-2"><i class="fa fa-check"></i><b>4.1.2</b> Batch effect correction</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-3"><i class="fa fa-check"></i><b>4.1.3</b> Clustering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-5.html"><a href="ch-5.html"><i class="fa fa-check"></i><b>5</b> Survey of deep learning models for scRNA-Seq analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-1"><i class="fa fa-check"></i><b>5.1</b> Imputation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-1"><i class="fa fa-check"></i><b>5.1.1</b> DCA:deep count autoencoder</a></li>
<li class="chapter" data-level="5.1.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-2"><i class="fa fa-check"></i><b>5.1.2</b> SAVER-X: single-cell analysis via expression recovery harnessing external data</a></li>
<li class="chapter" data-level="5.1.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-3"><i class="fa fa-check"></i><b>5.1.3</b> DeepImpute (Deep neural network Imputation)</a></li>
<li class="chapter" data-level="5.1.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-4"><i class="fa fa-check"></i><b>5.1.4</b> LATE: Learning with AuToEncoder</a></li>
<li class="chapter" data-level="5.1.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-5"><i class="fa fa-check"></i><b>5.1.5</b> scGMAI</a></li>
<li class="chapter" data-level="5.1.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-6"><i class="fa fa-check"></i><b>5.1.6</b> scIGANs</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-2"><i class="fa fa-check"></i><b>5.2</b> Batch effect correction</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-1"><i class="fa fa-check"></i><b>5.2.1</b> BERMUDA: Batch Effect ReMoval Using Deep Autoencoders</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-2"><i class="fa fa-check"></i><b>5.2.2</b> DESC: batch correction based on clustering</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-3"><i class="fa fa-check"></i><b>5.2.3</b> iMAP: Integration of Multiple single-cell datasets by Adversarial Paired-style transfer networks</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-3"><i class="fa fa-check"></i><b>5.3</b> Dimension reduction, latent representation, clustering, and data augmentation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-1"><i class="fa fa-check"></i><b>5.3.1</b> Dimension reduction by AEs with gene-interaction constrained architecture</a></li>
<li class="chapter" data-level="5.3.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-2"><i class="fa fa-check"></i><b>5.3.2</b> Dhaka: a VAE-based dimension reduction model</a></li>
<li class="chapter" data-level="5.3.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-3"><i class="fa fa-check"></i><b>5.3.3</b> cvis: a VAE for capturing low-dimensional structures</a></li>
<li class="chapter" data-level="5.3.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.3.4</b> scVAE: VAE for single-cell gene expression data</a></li>
<li class="chapter" data-level="5.3.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-5"><i class="fa fa-check"></i><b>5.3.5</b> VASC: VAE for scRNA-seq</a></li>
<li class="chapter" data-level="5.3.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-6"><i class="fa fa-check"></i><b>5.3.6</b> scDeepCluster</a></li>
<li class="chapter" data-level="5.3.7" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-7"><i class="fa fa-check"></i><b>5.3.7</b> cscGAN: Conditional single-cell generative adversarial neural networks</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-4"><i class="fa fa-check"></i><b>5.4</b> Multi-functional models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-4-1"><i class="fa fa-check"></i><b>5.4.1</b> scVI: single-cell variational inference</a></li>
<li class="chapter" data-level="5.4.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-4-2"><i class="fa fa-check"></i><b>5.4.2</b> LDVAE: linearly decoded variational autoencoder</a></li>
<li class="chapter" data-level="5.4.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-4-3"><i class="fa fa-check"></i><b>5.4.3</b> SAUCIE</a></li>
<li class="chapter" data-level="5.4.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-4-4"><i class="fa fa-check"></i><b>5.4.4</b> scScope</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-5"><i class="fa fa-check"></i><b>5.5</b> Doublet Classification</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-5-1"><i class="fa fa-check"></i><b>5.5.1</b> Solo</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-6"><i class="fa fa-check"></i><b>5.6</b> Automated cell type identification</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-6-1"><i class="fa fa-check"></i><b>5.6.1</b> DigitalDLSorter</a></li>
<li class="chapter" data-level="5.6.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-6-2"><i class="fa fa-check"></i><b>5.6.2</b> scCapsNet</a></li>
<li class="chapter" data-level="5.6.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-6-3"><i class="fa fa-check"></i><b>5.6.3</b> netAE: network-enhanced autoencoder</a></li>
<li class="chapter" data-level="5.6.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-6-4"><i class="fa fa-check"></i><b>5.6.4</b> scDGN - supervised adversarial alignment of single-cell RNA-seq data</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-5.html"><a href="ch-5.html#ch-5-7"><i class="fa fa-check"></i><b>5.7</b> Biological function prediction</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-7-1"><i class="fa fa-check"></i><b>5.7.1</b> CNNC: convolutional neural network for coexpression</a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-7-2"><i class="fa fa-check"></i><b>5.7.2</b> scGen, a generative model to predict perturbation response of single cells across cell types</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-6.html"><a href="ch-6.html"><i class="fa fa-check"></i><b>6</b> Conclusion and Discussions</a></li>
<li class="chapter" data-level="7" data-path="ch-7.html"><a href="ch-7.html"><i class="fa fa-check"></i><b>7</b> Tables</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-3" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Overview of common deep learning models for scRNA-seq analysis</h1>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure2"></span>
<img src="Figures/Figure2.png" alt="Graphical models of the surveyed DL models including A) Variational Autoencoder (VAE); B) Autoencoder (AE); and C) Generative Adversarial Network (GAN)" width="766" />
<p class="caption">
Figure 3.1: Graphical models of the surveyed DL models including A) Variational Autoencoder (VAE); B) Autoencoder (AE); and C) Generative Adversarial Network (GAN)
</p>
</div>
<p>Unsupervised learning is the key step in the scRNA-Seq analysis, including batch correction, dimension reduction, imputation, and clustering, which lend themselves naturally to unsupervised DL models including the variational autoencoder (VAE), the autoencoder (AE), or generative adversarial networks (GAN). Also, adversarial transfer learning has been applied for cell-type classification. We started our review by introducing the general formulations of VAE, AE, and GAN for scRNA-seq together with their training strategies. These general formulations facilitate understanding the methodologies used by different papers in developing their specific algorithms, enabling us to focus on the different features of each method and bring attention to their uniqueness and novelty.</p>

<div id="ch-3-1" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Variational Autoencoder (VAEs) for scRNA-Seq data</h2>
<p>Let <span class="math inline">\(x_{n}\)</span> represent a <span class="math inline">\(G \times 1\)</span> vector of gene expression (UMI counts or normalized, log-transformed expression) of <span class="math inline">\(G\)</span> genes in cell n, where <span class="math inline">\(x_{gn}\)</span> denotes gene <span class="math inline">\(g\)</span>’s expression, which is assumed to follow some distribution <span class="math inline">\(p(x_{gn} \vert v_{gn}, \alpha_{gn} )\)</span> (e.g., zero-inflated negative binomial (ZINB) or Gaussian), where <span class="math inline">\(v_{gn}\)</span> and <span class="math inline">\(\alpha_{gn}\)</span> are parameters of the distribution (e.g., mean, variance, or dispersion) (Fig.<a href="ch-3.html#fig:Figure2">3.1</a>A). We consider the first parameter <span class="math inline">\(v_{gn}\)</span> to be of particular interest (e.g., the mean counts) for the scRNA-seq analysis and is thus further modeled as a function of a d-dimension latent variable <span class="math inline">\(z_{n} \in R^{d}\)</span> and an observed variable <span class="math inline">\(s_{n}\)</span> (e.g., the batch ID) by a decoder neural network <span class="math inline">\(D_{\theta}\)</span> (Fig.<a href="ch-3.html#fig:Figure2">3.1</a>A) as</p>
<p><span class="math display" id="eq:eq1">\[\begin{equation}
v_{n} = D_{\theta}(z_{n}, s_{n}) \tag{3.1}
\end{equation}\]</span></p>
<p>where the <span class="math inline">\(g\)</span>th element of <span class="math inline">\(v_{n}\)</span> is <span class="math inline">\(v_{gn}\)</span> and <span class="math inline">\(\theta\)</span> is a vector of decoder weights, <span class="math inline">\(z_{n}\)</span> represents a low-dimensional latent representation of gene expression and is used in all the works for visualization and clustering. For VAE, <span class="math inline">\(z_{n}\)</span> is commonly assumed to follow a multivariate standard normal prior, i.e., <span class="math inline">\(p(z_{n})=N(0,I_{d})\)</span> with <span class="math inline">\(I_{d}\)</span> being a <span class="math inline">\(d \times d\)</span> identity matrix. The second parameter <span class="math inline">\(\alpha_{gn}\)</span> of <span class="math inline">\(p(x_{gn} \vert v_{gn}, \alpha_{gn})\)</span> is a nuisance parameter, which is assumed with a prior distribution <span class="math inline">\(p(\alpha_{gn})\)</span> and can be either estimated or marginalized in variational inference. Now define <span class="math inline">\(\Theta=\{\theta, \alpha_{ng}\forall{n},g\}\)</span> as the collection of the unknown model parameters. Then, <span class="math inline">\(p(x_{gn} \vert v_{gn}, \alpha_{gn})\)</span> and (<a href="ch-3.html#eq:eq1">(3.1)</a>) together define the likelihood <span class="math inline">\(p(x_{gn} \vert z_{n}, s_{gn}, \Theta)\)</span>.</p>
<p>The goal of training or inference is to compute the maximum likelihood estimate of </p>
<p><span class="math display" id="eq:eq2">\[\begin{equation}
    \hat{\Theta}_{ML} = argmax_{\Theta}\sum_{n=1}^{N}\log(x_{n} \vert s_{n},\Theta) \approx argmax_{\Theta}\sum_{n=1}^{N}\textit{L}(\Theta) \tag{3.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p(x_{n}\vert s_{n},\Theta)=\int(p(x_{n} \vert z_{n}, s_{n}, \Theta)p(z_{n})\textit{d}z_{n}\)</span> is the marginal likelihood, which is in general analytically intractable but can be lower-bounded by the evidence lower bound (ELBO) <span class="math inline">\(\textit{L}(\Theta)\)</span>, expressed as</p>
<p><span class="math display" id="eq:eq3">\[\begin{equation}
  \textit{L}(\Theta) = E_{q(z_{n} \vert x_{n},s_{n}, \Theta)}[\log{p(x_{n}\vert z_{n},s_{n}, \Theta)}] - D_{KL}[q(z_{n}\vert x_{n},s_{n},\Theta)\|p(z_{n})] \tag{3.3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(q(z_{n}│x_{n},s_{n})\)</span> is an approximate to the intractable posterior distribution <span class="math inline">\(p(z_{n}\vert x_{n},s_{n})\)</span>. To make the variational inference tractable <span class="math inline">\(q(z_{n}\vert x_{n},s_{n})\)</span> is assumed in most cases as a multivariate Gaussian</p>
<p><span class="math display" id="eq:eq4">\[\begin{equation}
  q(z_{n} \vert x_{n},s_{n})= N(\mu_{z_{n}},diag({\sigma_{Z_{n}}}^2)) \tag{3.4}
\end{equation}\]</span></p>
<p>whose means and variances <span class="math inline">\({\mu_{z_{n}},{\sigma_{Z_{n}}}^2}\)</span> are given by an encoder network <span class="math inline">\(E_{\phi}\)</span> applied to <span class="math inline">\(x_{n}\)</span> and <span class="math inline">\(s_{n}\)</span> (Fig.<a href="ch-3.html#fig:Figure2">3.1</a>A) as</p>
<p><span class="math display" id="eq:eq5">\[\begin{equation}
   {\mu_{z_{n}},{\sigma_{Z_{n}}}^2} = E_{\phi}(x_{n},s_{n}) \tag{3.5}
\end{equation}\]</span></p>
<p>where is the vector of the unknown decoder weights. Because of the approximation by <span class="math inline">\(\textit{L}(\Theta)\)</span> in (<a href="ch-3.html#eq:eq2">(3.2)</a>) and the introduction of the decoder network in eq.<a href="ch-3.html#eq:eq4">(3.4)</a>, the model parameters to be estimated become <span class="math inline">\(\theta=\{ \theta,\phi, \alpha_{ng}, \forall{n}, g \}\)</span>. Optimization of <span class="math inline">\(\textit{L}(\Theta)\)</span> in (<a href="ch-3.html#eq:eq2">(3.2)</a>) is computed efficiently by stochastic optimization, where the gradient is calculated by backpropagation.</p>
<p>All the surveyed papers that deploy VAE follow this general modeling process. However, an alternative and more general formulation treats it as optimization with a loss function expressed as</p>
<p><span class="math display" id="eq:eq6">\[\begin{equation}
L(\Theta) = - L(\Theta) + \sum_{k=1}^{K}\lambda_{k}L_{k}(\Theta)\tag{3.6}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(L_{k}\forall{k}=1,…,K\)</span> are <span class="math inline">\(K\)</span> additional function-specific losses introduced to constrain the model for different functions (clustering, cell type prediction, etc), and <span class="math inline">\(\lambda_{k}\)</span>s are the Lagrange multipliers. With this general formulation, for each paper, we focus on the survey the specific choice of data distribution <span class="math inline">\(p(x_{gn} \vert v_{gn},\alpha_{gn})\)</span> that defines <span class="math inline">\(L(\Theta)\)</span>, different <span class="math inline">\(L_{k}\)</span> designed for specific functions, and how the decoder and encoder are applied to model different aspects of scRNA-seq data.</p>

</div>
<div id="ch-3-2" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Autoencoders (AEs) for scRNA-seq data</h2>
<p>AEs have been proposed to learn the low dimensional latent representation of expression <span class="math inline">\(x_{n}\)</span>. The AE includes an encoder <span class="math inline">\(E_{\phi}\)</span> and a decoder <span class="math inline">\(D_{\theta}\)</span> (Fig.<a href="ch-3.html#fig:Figure2">3.1</a>B) such that</p>
<p><span class="math display" id="eq:eq7">\[\begin{equation}
    z_{n}=E_{\phi}(x_{n}); \hat{x_{n}} = D_{\theta}(z_{n})  \tag{3.7}
\end{equation}\]</span></p>
<p>where like VAE <span class="math inline">\(z_{n} \in R^{d}\)</span> is the d-dimension latent variable, <span class="math inline">\(\Theta = \{\theta,\phi\}\)</span> are encoder and decoder weight parameters, and <span class="math inline">\(\hat{x_{n}}\)</span> defines the parameters (e.g. mean) of data distribution and thus the likelihood <span class="math inline">\(p(x_{n} \vert \Theta)\)</span> (Fig.<a href="ch-2.html#fig:Figure1">2.1</a>B). Note that the mean of <span class="math inline">\(p(x_{n}\vert \Theta)\)</span> is often considered as the imputed and denoised expression of <span class="math inline">\(x_{n}\)</span>. For most common AEs, <span class="math inline">\(p(x_{n}\vert \Theta)\)</span> assumes a Gaussian distribution and <span class="math inline">\(\hat{x_{n}}\)</span> becomes the mean of the Gaussian and can be directly used as imputed, normalized gene expression. Nevertheless, additional designs can be introduced to attend imputation specifically. <span class="math inline">\(p(x_{n}\vert \Theta)\)</span> can also be negative binomial (NB) or ZINB as in DCA <span class="citation">(<a href="#ref-RN80" role="doc-biblioref">Chen, Ning, and Shi 2019</a>)</span><!--[23]--> to model the reads count directly with their parameters defined as functions of <span class="math inline">\(\hat{x_{n}}\)</span>. Additional design can be included in the AE model for batch correction, clustering, and other functions.</p>
<p>The training of the AE is generally carried out by stochastic gradient descent algorithms to minimize the loss with the general expression similar to that of VAE in eq.<a href="ch-3.html#eq:eq6">(3.6)</a></p>
<p><span class="math display" id="eq:eq8">\[\begin{equation}
    L(\Theta)=L_{0}(\Theta)+\sum_{k=1}^{K}\lambda_{k}L_{k}(\Theta) \tag{3.8}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(L_{0}\)</span> is <span class="math inline">\(-\log{p(x_{n}\vert \Theta)}\)</span>, and <span class="math inline">\(L_{k}s\)</span> are <span class="math inline">\(K\)</span> additional function-specific losses. When <span class="math inline">\(p(x_{n} \vert \Theta)\)</span> is the Gaussian, <span class="math inline">\(L_{0}\)</span> becomes one of the most commonly used mean square error (MSE) loss</p>
<p><span class="math display" id="eq:eq9">\[\begin{equation}
L_{0}(\Theta)=\sum_{n=1}^{N}\|x_{n}-\hat{x_{n}}\|_{2}^{2} \tag{3.9}
\end{equation}\]</span></p>
<p>Because different AE models differ in their AE architectures and the loss functions, we will discuss the specific architecture and the loss functions for each reviewed model.</p>

</div>
<div id="ch-3-3" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Generative adversarial networks (GANs) for scRNA-seq data</h2>
<p>GANs have been used for imputation, data generation and augmentation of the scRNA-seq analysis. Without loss of generality, the GAN, when applied to scRNA-seq, is designed to learn to generate gene expression profiles from <span class="math inline">\(p_{x}\)</span>, the distribution of <span class="math inline">\(x_{n}\)</span>, the count or normalized expression vectors of the scRNA data. The vanilla GAN consists of two deep neural networks <span class="citation">(<a href="#ref-RN98" role="doc-biblioref">Vitak et al. 2017</a>)</span><!--[2]-->. The first network is the generator <span class="math inline">\(G_{\theta}(z_{n},y_{n})\)</span> with parameter <span class="math inline">\(\theta\)</span>, which is essentially a decoder that takes a noise vector <span class="math inline">\(z_{n}\)</span> from the distribution <span class="math inline">\(p_{z}\)</span> and a class label <span class="math inline">\(y\)</span> (e.g. cell type) as input and is trained to generate <span class="math inline">\(x_{f}\)</span>, a “fake” sample of a gene expression profile (Fig.<a href="ch-3.html#fig:Figure2">3.1</a>C). Note that including a class label <span class="math inline">\(y_{n}\)</span> at the input is optional and when it is included, the model is known as the conditional GAN. The second network is the discriminator network <span class="math inline">\(D_{\phi_{D}}\)</span> with parameters <span class="math inline">\(\phi_{D}\)</span>, which is a classifier trained to distinguish between the “real” sample <span class="math inline">\(x\)</span> and fake data <span class="math inline">\(x_{f}\)</span> (Fig.<a href="ch-2.html#fig:Figure1">2.1</a>C). The generator <span class="math inline">\(G_{\theta}\)</span> and discriminator <span class="math inline">\(D_{\phi_{D}}\)</span> are trained to outplay each other, resulting in a minimax game, in which <span class="math inline">\(G_{\theta}\)</span> is forced by <span class="math inline">\(D_{\phi_{D}}\)</span> to produce better samples, which, when converge, can fool the discriminator <span class="math inline">\(D_{\phi_{D}}\)</span>, thus becoming samples from <span class="math inline">\(p_{x}\)</span>. The vanilla GAN suffers heavily from training instability and mode collapsing <span class="citation">(<a href="#ref-RN104" role="doc-biblioref">Rosenberg and Hirschberg 2007</a>)</span><!--[44]-->. To that end, Wasserstein GAN (WGAN) was developed to effectively improve the training stability and convergence <span class="citation">(<a href="#ref-RN107" role="doc-biblioref">Hubert and Arabie 1985</a>)</span><!--[45]-->. The WGAN loss computes the Wasserstein distance, also called the earth moving distance between the real and fake sample distributions <span class="citation">(<a href="#ref-RN181" role="doc-biblioref">Rousseeuw 1987</a>)</span><!--[46]-->:</p>
<p><span class="math display" id="eq:eq10">\[\begin{equation}
  L(\Theta)=\max_{\phi_{D}}\sum_{n=1}^{N}D_{\phi_{D}}(x_{n})-\sum{n=1}^{N}D_{\phi_{D}}(G_{\theta}(z_{n},y_{n})) \tag{3.10}
\end{equation}\]</span></p>
<p>Additional terms can also be added to (<a href="ch-3.html#eq:eq10">(3.10)</a>) to constrain the functions of the generator. Training based on the WGAN loss in Eq. (<a href="ch-3.html#eq:eq10">(3.10)</a> ) amounts to a min-max optimization, which iterates between the discriminator and generator, where each optimization is achieved by a stochastic gradient descent algorithm through backpropagation. The WGAN requires D to be K-Lipschitz continuous <span class="citation">(<a href="#ref-RN104" role="doc-biblioref">Rosenberg and Hirschberg 2007</a>)</span><!--[44]-->, which can be satisfied by adding the gradient penalty to the WGAN loss <span class="citation">(<a href="#ref-RN103" role="doc-biblioref">Strehland and Ghosh 2002</a>)</span><!--[47]-->. Once the training is done, the generator <span class="math inline">\(G_{\phi_{G}}\)</span> can be used to generate gene expression profiles of new cells.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-RN80" class="csl-entry">
Chen, G., B. Ning, and T. Shi. 2019. <span>“Single-Cell RNA-Seq Technologies and Related Computational Data Analysis.”</span> Journal Article. <em>Front Genet</em> 10: 317. <a href="https://doi.org/10.3389/fgene.2019.00317">https://doi.org/10.3389/fgene.2019.00317</a>.
</div>
<div id="ref-RN107" class="csl-entry">
Hubert, L., and P. Arabie. 1985. <span>“Comparing Partitions.”</span> Journal Article. <em>Journal of Classification</em> 2: 193–218.
</div>
<div id="ref-RN104" class="csl-entry">
Rosenberg, A., and J. Hirschberg. 2007. <span>“Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).”</span> Journal Article, 410–20.
</div>
<div id="ref-RN181" class="csl-entry">
Rousseeuw, P. J. 1987. <span>“Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.”</span> Journal Article. <em>Journal of Computational and Applied Mathematics</em> 20: 583–617.
</div>
<div id="ref-RN103" class="csl-entry">
Strehland, A., and J. Ghosh. 2002. <span>“Cluster Ensembles—a Knowledge Reuse Framework for Combining Multiple Partitions.”</span> Journal Article. <em>J Mach Learn Res</em> 3: 583–617.
</div>
<div id="ref-RN98" class="csl-entry">
Vitak, S. A., K. A. Torkenczy, J. L. Rosenkrantz, A. J. Fields, L. Christiansen, M. H. Wong, L. Carbone, F. J. Steemers, and A. Adey. 2017. <span>“Sequencing Thousands of Single-Cell Genomes with Combinatorial Indexing.”</span> Journal Article. <em>Nat Methods</em> 14 (3): 302–8. <a href="https://doi.org/10.1038/nmeth.4154">https://doi.org/10.1038/nmeth.4154</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Huang-AI4Medicine-Lab/survey-of-DL-for-scRNA-seq-analysis/edit/main/03-overview-of-common-dl-models-for-scRNA-seq-analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["survey-of-DL-for-scRNA-seq-analysis.pdf", "survey-of-DL-for-scRNA-seq-analysis.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
