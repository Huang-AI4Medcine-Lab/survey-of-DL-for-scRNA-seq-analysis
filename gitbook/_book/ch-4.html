<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Overview of datasets and evaluation metrics | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis</title>
  <meta name="description" content="4 Overview of datasets and evaluation metrics | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Overview of datasets and evaluation metrics | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Huang-AI4Medicine-Lab/survey-of-DL-for-scRNA-seq-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Overview of datasets and evaluation metrics | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  
  
  

<meta name="author" content="Mario Flores1§, Zhentao Liu1, Tinghe Zhang1, Md Musaddaqui Hasib1, Yu-Chiao Chiu2, Zhenqing Ye2,3, Karla Paniagua1, Sumin Jo1, Jianqiu Zhang1, Shou-Jiang Gao4,6, Yufang Jin1, Yidong Chen2,3§, and Yufei Huang5,6§" />


<meta name="date" content="2021-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-3.html"/>
<link rel="next" href="ch-5.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Survey of Deep Learning for scRNA-seq Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the authors</a></li>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html"><i class="fa fa-check"></i>About this book</a>
<ul>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html#key-points"><i class="fa fa-check"></i>Key Points</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-1.html"><a href="ch-1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="ch-2.html"><a href="ch-2.html"><i class="fa fa-check"></i><b>2</b> Overview of scRNA-seq processing pipeline</a></li>
<li class="chapter" data-level="3" data-path="ch-3.html"><a href="ch-3.html"><i class="fa fa-check"></i><b>3</b> Overview of common deep learning models for scRNA-seq analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-3.html"><a href="ch-3.html#ch-3-1"><i class="fa fa-check"></i><b>3.1</b> Variational Autoencoder (VAEs) for scRNA-Seq data</a></li>
<li class="chapter" data-level="3.2" data-path="ch-3.html"><a href="ch-3.html#ch-3-2"><i class="fa fa-check"></i><b>3.2</b> Autoencoders (AEs) for scRNA-seq data</a></li>
<li class="chapter" data-level="3.3" data-path="ch-3.html"><a href="ch-3.html#ch-3-3"><i class="fa fa-check"></i><b>3.3</b> Generative adversarial networks (GANs) for scRNA-seq data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-4.html"><a href="ch-4.html"><i class="fa fa-check"></i><b>4</b> Overview of datasets and evaluation metrics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-4.html"><a href="ch-4.html#ch-4-1"><i class="fa fa-check"></i><b>4.1</b> Evaluation methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-1"><i class="fa fa-check"></i><b>4.1.1</b> Imputation</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-2"><i class="fa fa-check"></i><b>4.1.2</b> Batch effect correction</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-3"><i class="fa fa-check"></i><b>4.1.3</b> Clustering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-5.html"><a href="ch-5.html"><i class="fa fa-check"></i><b>5</b> Survey of deep learning models for scRNA-Seq analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-1"><i class="fa fa-check"></i><b>5.1</b> Imputation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-1"><i class="fa fa-check"></i><b>5.1.1</b> DCA:deep count autoencoder</a></li>
<li class="chapter" data-level="5.1.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-2"><i class="fa fa-check"></i><b>5.1.2</b> SAVER-X: single-cell analysis via expression recovery harnessing external data</a></li>
<li class="chapter" data-level="5.1.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-3"><i class="fa fa-check"></i><b>5.1.3</b> DeepImpute (Deep neural network Imputation)</a></li>
<li class="chapter" data-level="5.1.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-4"><i class="fa fa-check"></i><b>5.1.4</b> LATE: Learning with AuToEncoder</a></li>
<li class="chapter" data-level="5.1.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-5"><i class="fa fa-check"></i><b>5.1.5</b> scGMAI</a></li>
<li class="chapter" data-level="5.1.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-6"><i class="fa fa-check"></i><b>5.1.6</b> scIGANs</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-2"><i class="fa fa-check"></i><b>5.2</b> Batch effect correction</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-1"><i class="fa fa-check"></i><b>5.2.1</b> BERMUDA: Batch Effect ReMoval Using Deep Autoencoders</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-2"><i class="fa fa-check"></i><b>5.2.2</b> DESC: batch correction based on clustering</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-3"><i class="fa fa-check"></i><b>5.2.3</b> iMAP: Integration of Multiple single-cell datasets by Adversarial Paired-style transfer networks</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-3"><i class="fa fa-check"></i><b>5.3</b> Dimension reduction, latent representation, clustering, and data augmentation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-1"><i class="fa fa-check"></i><b>5.3.1</b> Dimension reduction by AEs with gene-interaction constrained architecture</a></li>
<li class="chapter" data-level="5.3.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-2"><i class="fa fa-check"></i><b>5.3.2</b> Dhaka: a VAE-based dimension reduction model</a></li>
<li class="chapter" data-level="5.3.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-3"><i class="fa fa-check"></i><b>5.3.3</b> cvis: a VAE for capturing low-dimensional structures</a></li>
<li class="chapter" data-level="5.3.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.3.4</b> scVAE: VAE for single-cell gene expression data</a></li>
<li class="chapter" data-level="5.3.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-5"><i class="fa fa-check"></i><b>5.3.5</b> VASC: VAE for scRNA-seq</a></li>
<li class="chapter" data-level="5.3.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-6"><i class="fa fa-check"></i><b>5.3.6</b> scDeepCluster</a></li>
<li class="chapter" data-level="5.3.7" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-7"><i class="fa fa-check"></i><b>5.3.7</b> cscGAN: Conditional single-cell generative adversarial neural networks</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-4"><i class="fa fa-check"></i><b>5.4</b> Multi-functional models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-4-1"><i class="fa fa-check"></i><b>5.4.1</b> scVI: single-cell variational inference</a></li>
<li class="chapter" data-level="5.4.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-4-2"><i class="fa fa-check"></i><b>5.4.2</b> LDVAE: linearly decoded variational autoencoder</a></li>
<li class="chapter" data-level="5.4.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-4-3"><i class="fa fa-check"></i><b>5.4.3</b> SAUCIE</a></li>
<li class="chapter" data-level="5.4.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-4-4"><i class="fa fa-check"></i><b>5.4.4</b> scScope</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-5"><i class="fa fa-check"></i><b>5.5</b> Doublet Classification</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-5-1"><i class="fa fa-check"></i><b>5.5.1</b> Solo</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-6"><i class="fa fa-check"></i><b>5.6</b> Automated cell type identification</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-6-1"><i class="fa fa-check"></i><b>5.6.1</b> DigitalDLSorter</a></li>
<li class="chapter" data-level="5.6.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-6-2"><i class="fa fa-check"></i><b>5.6.2</b> scCapsNet</a></li>
<li class="chapter" data-level="5.6.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-6-3"><i class="fa fa-check"></i><b>5.6.3</b> netAE: network-enhanced autoencoder</a></li>
<li class="chapter" data-level="5.6.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-6-4"><i class="fa fa-check"></i><b>5.6.4</b> scDGN - supervised adversarial alignment of single-cell RNA-seq data</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-5.html"><a href="ch-5.html#ch-5-7"><i class="fa fa-check"></i><b>5.7</b> Biological function prediction</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-7-1"><i class="fa fa-check"></i><b>5.7.1</b> CNNC: convolutional neural network for coexpression</a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-7-2"><i class="fa fa-check"></i><b>5.7.2</b> scGen, a generative model to predict perturbation response of single cells across cell types</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-6.html"><a href="ch-6.html"><i class="fa fa-check"></i><b>6</b> Conclusion and Discussions</a></li>
<li class="chapter" data-level="7" data-path="ch-7.html"><a href="ch-7.html"><i class="fa fa-check"></i><b>7</b> Tables</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-4" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Overview of datasets and evaluation metrics</h1>
<p>A variety of datasets and performance evaluation metrics have been used to demonstrate the performance of the surveyed DL models for different tasks. We summarize these datasets and evaluation metrics in Table <a href="ch-7.html#fig:Table2a">7.2</a> &amp; <a href="ch-7.html#fig:Table3">7.7</a>. We detail the mathematical definition of the evaluation metrics in the following.</p>

<div id="ch-4-1" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Evaluation methods</h2>
<p>An extensive list of evaluation methods has been proposed for different scRNA-seq analysis tasks <span class="citation">(<a href="#ref-RN44" role="doc-biblioref">Tran et al. 2020</a>; <a href="#ref-RN45" role="doc-biblioref">Hou et al. 2020</a>; <a href="#ref-RN43" role="doc-biblioref">Sun and Zhou 2019</a>)</span><!--[18, 48, 49]-->. We provide an overview here the methods adopted in the surveyed papers. We discuss them according to the key categories on which the surveyed papers are organized, namely, imputation, batch effect correction, dimension reduction and clustering, cell type identification, and functional analysis.</p>

<div id="ch-4-1-1" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Imputation</h3>
<p>The evaluation of the performance of imputation methods considers their ability to recover biological signals and improve downstream analyses. For this two main methods have been used. First is the evaluation of similarity between bulk and imputed scRNA-seq data. Second is the evaluation of imputation on unsupervised clustering.</p>
<p>The first approach consist in assessing the similarity between bulk and imputed scRNA-seq data. For a given scRNA-seq dataset, the “pseudobulk,” or the average of normalized (log2-transformed) scRNA-seq counts across cells, is calculated first, and the Spearman’s rank correlation coefficient (SCC) between the pseudobulk and the bulk RNA-seq profile of the same cell type is evaluated. The statistical significance is assessed whether SCCs bewteen the bulk and pseudobulks from two imputation methods are equal.</p>
<p>The second approach consist in measuring the accuracy of several clustering assignments methods using four metrics:</p>
<ul>
<li>Entropy of accuracy (<span class="math inline">\(H_{acc}\)</span>) and entropy of purity (<span class="math inline">\(H_{pur}\)</span>). Hacc (<span class="math inline">\(H_{pur}\)</span>) measures the diversity of the ground-truth labels (predicted cluster labels) within each predicted cluster group (ground-truth group), respectively.</li>
</ul>
<p><span class="math display">\[H_{acc}=-1\frac{1}{M}\sum_{i=1}^{M}\sum_{j=1}^{N_{i}}p_{i}(x_{j})\log{p_{i}(x_{j})}\]</span></p>
<p><span class="math display">\[H_{pur}=-1\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M_{i}}q_{i}(x_{j})\log{q_{i}(x_{j})}\]</span></p>
<p>where <span class="math inline">\(M\)</span> is the total number of predicted clusters from the clustering algorithm, <span class="math inline">\(N\)</span> is the number of ground-truth clusters, <span class="math inline">\(M_{i}\)</span>, (or <span class="math inline">\(N_{i}\)</span>) is the number of predicted clusters (or ground-truth clusters) in the ith ground -truth cluster (or predicted cluster), respectively. <span class="math inline">\(p_{i}(x_{j})\)</span> (or <span class="math inline">\(q_{i}(x_{j}))\)</span> are the proportions of cells in the <span class="math inline">\(j\)</span>th ground-truth cluster (or predicted cluster) relative to the total number of cells in the <span class="math inline">\(i\)</span>th predicted cluster (or ground-truth clusters), respectively. A smaller value of <span class="math inline">\(H_{acc}\)</span> means the cells in a predicted cluster are constantly labeled from the same ground-truth group, while A smaller value of <span class="math inline">\(H_{pur}\)</span> means the cells in the ground-truth groups are homogeneous with the same predicted cluster labels <span class="citation">(<a href="#ref-RN44" role="doc-biblioref">Tran et al. 2020</a>)</span><!--[18]-->. However, smaller <span class="math inline">\(H_{acc}\)</span> (or <span class="math inline">\(H_{pur}\)</span> ) can lead to over-clustering (or under-cluster), when each predicted cluster contains 1 cell (<span class="math inline">\(H_{acc}\)</span> = 0) or all cells in one predicted cluster (<span class="math inline">\(H_{pur}\)</span> = 0).</p>
<ul>
<li>Adjusted Rand index (ARI). Rand index (RI) is another measure of constancy between two clustering outcomes. If <span class="math inline">\(a\)</span> (or <span class="math inline">\(b\)</span>) is the count of number of pairs of cells in one cluster (or different clusters) from one clustering algorithm but also fall in the same cluster (or different clusters) from the other clustering algorithm, then, <span class="math inline">\(RI=(a+b)/{\binom{n}{2}}\)</span>, where <span class="math inline">\(\binom{n}{2}\)</span> is the total number of pairs when given n cells. The RI has a value between 0 and 1, with 0 indicating that the two clustering algorithms do not agree on any pair of cells and 1 indicating that the two clustering algorithms are exactly the same. ARI is a corrected-for-chance version of <span class="math inline">\(RI\)</span>, or</li>
</ul>
<p><span class="math display">\[ARI = \frac{RI-E[RI]}{\max(RI)-E[RI]}\]</span>
where <span class="math inline">\(E[RI]\)</span> is the expected Rand Index <span class="citation">(<a href="#ref-RN107" role="doc-biblioref">Hubert and Arabie 1985</a>)</span><!--[50]-->.</p>
<ul>
<li>Median Silhouette index. The Silhouette index is defined as</li>
</ul>
<p><span class="math display">\[ s(i)=\frac{b(i)-a(i)}{\max(a(i),b(i))}\]</span></p>
<p>where <span class="math inline">\(a(i)\)</span> is the average dissimilarity of <span class="math inline">\(i\)</span>th cell to all other cells in the same cluster, and <span class="math inline">\(b(i)\)</span> is the average dissimilarity of <span class="math inline">\(i\)</span>th cell to all cells in the closest cluster. The range of <span class="math inline">\(s(i)\)</span> is [−1,1], with 1 to be well-clustered with appropriate labels, and -1 to be completely misclassified. <span class="math inline">\(s(i) = 0\)</span> indiates the cell could be assigned to nearest clusters (or overlapping clusters).</p>
<p>A good imputation method should allow perform downstream (clustering) analyses without introducing any artifacts or false signals.</p>

</div>
<div id="ch-4-1-2" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Batch effect correction</h3>
<p>When evaluating the performance of a batch correction method, we need to consider how well it mixes the shared cell types between different batches and at the same time identifies batch-specific cells. The existing metrics can be classified as cluster-level and cell-level metrics. Cluster level metrics are those used for evaluating clustering performance and include adjusted rand index (ARI), normalized mutual information (NMI), and silhouette coefficients. They are easy to compute but do not measure local mixture of cells from different batches. This drawback is addressed by the cell-level metrics, which includes k-nearest neighbor batch-effect test (kBET), local inverse Simpson’s index (LISI), and classifier-based. Because the cluster-level metrics will be discussed in detail in Section <a href="#ch-4-2-3"><strong>??</strong></a>, we focus on discussing cell-level metrics in this section.</p>
<p>Entropy of mixing. This metric evaluates the mixing of cells from different batches in the neighborhood of each cell <span class="citation">(<a href="#ref-RN84" role="doc-biblioref">Haghverdi et al. 2018</a>)</span><!--[28]-->. It first randomly sample N cells and then for each cell it calculates the regional entropy of mixing as</p>
<p><span class="math display">\[E = \sum_{i=1}^{C}p_{i}\log{(p_{i})}\]</span></p>
<p>where <span class="math inline">\(C\)</span> is the number of batches and <span class="math inline">\(p_{i}\)</span> is the proportion of cells from batch <span class="math inline">\(i\)</span> among <span class="math inline">\(N\)</span> nearest cells (e.g. <span class="math inline">\(N\)</span> = 100). The total entropy is the sum of reginal entropies. The computation repeats <span class="math inline">\(K\)</span> times to obtain an empirical distribution of the entropy of mixing.</p>
<p><strong>Maximum Mean Discrepancy (MMD)</strong> is a non-parametric distance between distributions based on the reproducing kernel Hilbert space (RKHS) <span class="citation">(<a href="#ref-RN185" role="doc-biblioref">Borgwardt et al. 2006</a>)</span><!--[51]-->, or, MMD is a distance-based measure between two distribution <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> based on the mean embeddings <span class="math inline">\(\mu_{p}\)</span> and <span class="math inline">\(\mu_{q}\)</span> in a reproducing kernel Hilbert space <span class="math inline">\(F\)</span>,</p>
<p><span class="math display">\[MMD(F,p,q)=\sup_{f \in F}\|\mu_{p}-\mu_{q}\|_{f}\]</span></p>
<p>MMD will vanish for most finite samples <span class="math inline">\(x_{k}\)</span> and <span class="math inline">\(y_{k}\)</span> only if two distributions are the same.</p>
<p><strong>k-Nearest neighbor batch-effect test (kBET)</strong>. kBET assesses the batch mixing by comparing the batch-specific distribution within <span class="math inline">\(k\)</span>-nearest neighbors (kNNs) of a cell with the global distribution of batches <span class="citation">(<a href="#ref-RN105" role="doc-biblioref">Buttner et al. 2019</a>)</span><!--[52]-->. It uses a <span class="math inline">\(X^2\)</span>-based test for random neighborhoods of fixed size to determine whether they are well mixed. Given a dataset of <span class="math inline">\(N\)</span> cells from <span class="math inline">\(L\)</span> batches with <span class="math inline">\(N_{l}\)</span> denoting the number of cells in batch <span class="math inline">\(l\)</span>. Under the null hypothesis that cells are ‘well mixed,’ that is the absence of batch effect, we have the test statistics as</p>
<p><span class="math display">\[a_{n}^{k} = \sum_{l=1}^{L}\frac{(N_{nl}^{k}-k*f_{l})^{2}}{k*f_{l}} \sim X^{2}_{L-1}\]</span></p>
<p>where <span class="math inline">\(N_{nl}^{k}\)</span> is the number of cells from batch <span class="math inline">\(l\)</span> in the <span class="math inline">\(k\)</span>-nearest neighbors of cell <span class="math inline">\(n\)</span>, <span class="math inline">\(f_{l}\)</span> is the global fraction of cells in batch <span class="math inline">\(l\)</span>, or <span class="math inline">\(f_{l}=\frac{N_{l}}{N}\)</span>, and <span class="math inline">\(X_{L-1}^2\)</span> denotes the <span class="math inline">\(X^2\)</span> distribution with <span class="math inline">\(L-1\)</span> degrees of freedom. The averaged rejection rate of the <span class="math inline">\(Χ^2\)</span> test for all cells is used to define the performance of a batch correction method.</p>
<p><strong>Local Inverse Simpson’s Index (LISI).</strong> Like kBET, LISI also compares the batch mixing at local level with global batch distribution. However, unlike kBET, which is agnostic of cell types, LISI requires well mixing of cells from the same cell type but not of those from different types <span class="citation">(<a href="#ref-RN74" role="doc-biblioref">Korsunsky et al. 2019</a>)</span><!--[30]-->. LISI evaluates cell-type-specific mixing using an inverse Simpson’s Index in a local neighborhood of each cell. LISI builds Gaussian kernel-based distributions of local neighborhoods sensitive to local diversity. It calculates inverse Simpson’s Index in the k-nearest neighbors of cell <span class="math inline">\(n\)</span> for all batches as <span class="math inline">\(\frac{1}{\lambda(n)}=\frac{1}{\sum_{l=1}^{L}(p(l))^{2}}\)</span>, where <span class="math inline">\(p(l)\)</span> denotes the proportion of batch <span class="math inline">\(l\)</span> in the <span class="math inline">\(k\)</span>-nearest neighbors. The score reports the effective number of batches in the <span class="math inline">\(k\)</span>-nearest neighbors of cell <span class="math inline">\(n\)</span>. Inverse Simpson’s Index in the <span class="math inline">\(k\)</span>-nearest neighbors of cell <span class="math inline">\(n\)</span> can also be calculated to evaluate the diversity of different cell types. However, in an ideal case, LISI score should be 1, reflecting a separation of unique cell types.</p>
<p><strong>Classifier-based.</strong> Although LISI addresses the issue of cell-type proportion of different batches but it is hard to summarize all single cell-level LISI scores into a simple statistic for comparing across different methods <span class="citation">(<a href="#ref-RN53" role="doc-biblioref">Eraslan et al. 2019</a>)</span><!--[15]-->. The classifier-based approach addresses this issue by using two distinct local classifiers for each single cell. The first classifier classifies every single cell as positive and negative cells. A single cell <span class="math inline">\(n\)</span> is positive if at least 50<span class="math inline">\(%\)</span> cells of its k-nearest neighbor (KNN) cells are from the same cell-type label, otherwise ‘negative.’ The positive cells are further classified into true and false positive cells, where true positive cells are those surrounded by appropriate portions of cells with L batches. In other words, if we sample <span class="math inline">\(k\)</span> cells from this cell-type cluster, the expected number of cells from batch <span class="math inline">\(l\)</span> will be <span class="math inline">\(k*f_{l}\)</span>, where <span class="math inline">\(f_{l}\)</span> is the global fraction of cells in batch <span class="math inline">\(l\)</span>. A positive cell in this cluster is a true positive when the observed cell numbers for each batch among its <span class="math inline">\(k\)</span> neighbors are within 3 standard deviations around the expected numbers. The proportions of positive cells and true positive cells are used as the summary metrics to evaluate the overall performance of batch effect removal. The higher the proportions, the better the algorithm.</p>

</div>
<div id="ch-4-1-3" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Clustering</h3>
<p>Evaluating the performance of clustering algorithms is not as trivial as counting the number of errors like supervised learning. In general, the clustering performance evaluation metric should not just take absolute corrected labelled cells into account but also consider if the clustering defines a good similarity or separation in the dataset compared to ground truth. When ground truth is not available, evaluation must be performed using model itself such as clustering distance, dispersion, etc. Similar measures, such as Adjusted Rand Index (ARI) and Silhouette Index discussed in Section <a href="#ch-4-2-1"><strong>??</strong></a> can also be employed here to measure the agreement between predicted assignment to the ground-truth assignment.</p>
<p><strong>Normalized Mutual Information (NMI).</strong> The mutual information(MI) [47] is a measure of mutual dependency between two cluster assignments <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>. It quantifies the amount of information we could have about one assignment by observing the other assignment. For <span class="math inline">\(N\)</span> samples, we have the entropy for cluster assignments <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> as</p>
<p><span class="math display">\[H(U)=\sum_{i=1}^{\vert U \vert}P_{U}(i)\log{(P_{U}(i))},  H(V)=\sum_{i=1}^{\vert V \vert}P_{V}(i)\log{(P_{V}(i))}\]</span></p>
<p>where <span class="math inline">\(P_{U}(i)=\frac{\vert U_{i} \vert}{N}\)</span> and <span class="math inline">\(P_{V}(j)=\frac{\vert V_{j} \vert}{N}\)</span>. Also, define the joint distribution probability is <span class="math inline">\(P_{UV}(i,j)=\frac{\vert U_{i}\cap V_{j} \vert}{N}\)</span>. Then, the mutual information of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> is defined as</p>
<p><span class="math display">\[MI(U,V)=\sum_{i=1}^{\vert U \vert}\sum_{j=1}^{\vert V \vert}P_{UV}(i,j)\log{\frac{P_{UV}(i,j)}{P_{U}(i)P_{V}(j)}}\]</span></p>
<p>The NMI is a normalization of the <span class="math inline">\(MI\)</span> score between 0 and 1. For example, the average NMI is defined as <span class="citation">(<a href="#ref-RN250" role="doc-biblioref">Cover 1999</a>)</span><!--[53]--></p>
<p><span class="math display">\[NMI(U,V)=\frac{2 \times MI(U,V)}{[H(U) + H(V)]}\]</span></p>
<p><strong>Homogeneity, Completeness, and V-Measure.</strong> The homogeneity score (HS) measures the extent to which its clusters contain only samples that belong to one cell type, or <span class="math inline">\(HS=1-H(P(U\vert V))/H(P(U))\)</span>, where <span class="math inline">\(H()\)</span> is the entropy, and <span class="math inline">\(U\)</span> is the ground-truth assignment and <span class="math inline">\(V\)</span> is the predicted assignment. The <span class="math inline">\(HS\)</span> range from 0 to 1, where 1 indicates perfectly homogeneous labelling. Similarly, the completeness score (CS) is defined as <span class="math inline">\(CS=1-H(P(V \vert U))/H(P(V))\)</span>, its values range from 0 to 1, where 1 indicates all member from a ground-truth label are assigned to a single cluster.</p>
<p>The V-Measure [54<em>(Reference Not Found)</em>]<!--[54]--> is the harmonic mean between <span class="math inline">\(HS\)</span> and <span class="math inline">\(CS\)</span>, defined as <span class="math inline">\(V_{\beta}=\frac{(1+\beta)HS×CS}{\beta HC+CS}\)</span>, where <span class="math inline">\(\beta\)</span> indicates the weight of <span class="math inline">\(HS\)</span>. V-Measure is a more symmetric, i.e. switching the true and predicted cluster labels does not change V-Measure.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-RN185" class="csl-entry">
Borgwardt, K. M., A. Gretton, M. J. Rasch, H. P. Kriegel, B. Scholkopf, and A. J. Smola. 2006. <span>“Integrating Structured Biological Data by Kernel Maximum Mean Discrepancy.”</span> Journal Article. <em>Bioinformatics</em> 22 (14): e49–57. <a href="https://doi.org/10.1093/bioinformatics/btl242">https://doi.org/10.1093/bioinformatics/btl242</a>.
</div>
<div id="ref-RN105" class="csl-entry">
Buttner, M., Z. Miao, F. A. Wolf, S. A. Teichmann, and F. J. Theis. 2019. <span>“A Test Metric for Assessing Single-Cell RNA-Seq Batch Correction.”</span> Journal Article. <em>Nat Methods</em> 16 (1): 43–49. <a href="https://doi.org/10.1038/s41592-018-0254-1">https://doi.org/10.1038/s41592-018-0254-1</a>.
</div>
<div id="ref-RN250" class="csl-entry">
Cover, Thomas M. 1999. <em>Elements of Information Theory</em>. Book. John Wiley &amp; Sons.
</div>
<div id="ref-RN53" class="csl-entry">
Eraslan, G., L. M. Simon, M. Mircea, N. S. Mueller, and F. J. Theis. 2019. <span>“Single-Cell RNA-Seq Denoising Using a Deep Count Autoencoder.”</span> Journal Article. <em>Nat Commun</em> 10 (1): 390. <a href="https://doi.org/10.1038/s41467-018-07931-2">https://doi.org/10.1038/s41467-018-07931-2</a>.
</div>
<div id="ref-RN84" class="csl-entry">
Haghverdi, L., A. T. L. Lun, M. D. Morgan, and J. C. Marioni. 2018. <span>“Batch Effects in Single-Cell RNA-Sequencing Data Are Corrected by Matching Mutual Nearest Neighbors.”</span> Journal Article. <em>Nat Biotechnol</em> 36 (5): 421–27. <a href="https://doi.org/10.1038/nbt.4091">https://doi.org/10.1038/nbt.4091</a>.
</div>
<div id="ref-RN45" class="csl-entry">
Hou, W., Z. Ji, H. Ji, and S. C. Hicks. 2020. <span>“A Systematic Evaluation of Single-Cell RNA-Sequencing Imputation Methods.”</span> Journal Article. <em>Genome Biol</em> 21 (1): 218. <a href="https://doi.org/10.1186/s13059-020-02132-x">https://doi.org/10.1186/s13059-020-02132-x</a>.
</div>
<div id="ref-RN107" class="csl-entry">
Hubert, L., and P. Arabie. 1985. <span>“Comparing Partitions.”</span> Journal Article. <em>Journal of Classification</em> 2: 193–218.
</div>
<div id="ref-RN74" class="csl-entry">
Korsunsky, I., N. Millard, J. Fan, K. Slowikowski, F. Zhang, K. Wei, Y. Baglaenko, M. Brenner, P. R. Loh, and S. Raychaudhuri. 2019. <span>“Fast, Sensitive and Accurate Integration of Single-Cell Data with Harmony.”</span> Journal Article. <em>Nat Methods</em> 16 (12): 1289–96. <a href="https://doi.org/10.1038/s41592-019-0619-0">https://doi.org/10.1038/s41592-019-0619-0</a>.
</div>
<div id="ref-RN43" class="csl-entry">
Sun, Zhu, S., and X. Zhou. 2019. <span>“Accuracy, Robustness and Scalability of Dimensionality Reduction Methods for Single-Cell RNA-Seq Analysis.”</span> Journal Article. <em>Genome Biol</em> 20 (1): 269. <a href="https://doi.org/10.1186/s13059-019-1898-6">https://doi.org/10.1186/s13059-019-1898-6</a>.
</div>
<div id="ref-RN44" class="csl-entry">
Tran, H. T. N., K. S. Ang, M. Chevrier, X. Zhang, N. Y. S. Lee, M. Goh, and J. Chen. 2020. <span>“A Benchmark of Batch-Effect Correction Methods for Single-Cell RNA Sequencing Data.”</span> Journal Article. <em>Genome Biol</em> 21 (1): 12. <a href="https://doi.org/10.1186/s13059-019-1850-9">https://doi.org/10.1186/s13059-019-1850-9</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Huang-AI4Medicine-Lab/survey-of-DL-for-scRNA-seq-analysis/edit/main/04-overview-of-datasets-and-evaluation-metrics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["survey-of-DL-for-scRNA-seq-analysis.pdf", "survey-of-DL-for-scRNA-seq-analysis.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
