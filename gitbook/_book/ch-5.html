<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Survey of deep learning models for scRNA-Seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis</title>
  <meta name="description" content="5 Survey of deep learning models for scRNA-Seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Survey of deep learning models for scRNA-Seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Huang-AI4Medicine-Lab/survey-of-DL-for-scRNA-seq-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Survey of deep learning models for scRNA-Seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  
  
  

<meta name="author" content="Mario Flores, Zhentao Liu, Tinghe Zhang, Md Musaddaqui Hasib, Yu-Chiao Chiu, Zhenqing Ye, Karla Paniagua, Sumin Jo, Jianqiu Zhang, Shou-Jiang Gao, Yufang Jin, Yidong Chen, and Yufei Huang" />


<meta name="date" content="2021-09-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-4.html"/>
<link rel="next" href="ch-6.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Survey of Deep Learning for scRNA-seq Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html"><i class="fa fa-check"></i>About this book</a>
<ul>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html#key-points"><i class="fa fa-check"></i>Key Points</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-1.html"><a href="ch-1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="ch-2.html"><a href="ch-2.html"><i class="fa fa-check"></i><b>2</b> Overview of scRNA-seq processing pipeline</a></li>
<li class="chapter" data-level="3" data-path="ch-3.html"><a href="ch-3.html"><i class="fa fa-check"></i><b>3</b> Overview of common deep learning models for scRNA-seq analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-3.html"><a href="ch-3.html#ch-3-1"><i class="fa fa-check"></i><b>3.1</b> Variational Autoencoder (VAEs) for scRNA-Seq data</a></li>
<li class="chapter" data-level="3.2" data-path="ch-3.html"><a href="ch-3.html#ch-3-2"><i class="fa fa-check"></i><b>3.2</b> Autoencoders (AEs) for scRNA-seq data</a></li>
<li class="chapter" data-level="3.3" data-path="ch-3.html"><a href="ch-3.html#ch-3-3"><i class="fa fa-check"></i><b>3.3</b> Generative adversarial networks (GANs) for scRNA-seq data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-4.html"><a href="ch-4.html"><i class="fa fa-check"></i><b>4</b> Overview of datasets and evaluation metrics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-4.html"><a href="ch-4.html#ch-4-1"><i class="fa fa-check"></i><b>4.1</b> Evaluation methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-1"><i class="fa fa-check"></i><b>4.1.1</b> Imputation</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-2"><i class="fa fa-check"></i><b>4.1.2</b> Batch effect correction</a></li>
<li class="chapter" data-level="4.1.3" data-path="ch-4.html"><a href="ch-4.html#ch-4-1-3"><i class="fa fa-check"></i><b>4.1.3</b> Clustering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-5.html"><a href="ch-5.html"><i class="fa fa-check"></i><b>5</b> Survey of deep learning models for scRNA-Seq analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-1"><i class="fa fa-check"></i><b>5.1</b> Imputation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-1"><i class="fa fa-check"></i><b>5.1.1</b> DCA:deep count autoencoder</a></li>
<li class="chapter" data-level="5.1.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-2"><i class="fa fa-check"></i><b>5.1.2</b> SAVER-X: single-cell analysis via expression recovery harnessing external data</a></li>
<li class="chapter" data-level="5.1.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-3"><i class="fa fa-check"></i><b>5.1.3</b> DeepImpute (Deep neural network Imputation)</a></li>
<li class="chapter" data-level="5.1.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-4"><i class="fa fa-check"></i><b>5.1.4</b> LATE: Learning with AuToEncoder</a></li>
<li class="chapter" data-level="5.1.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-5"><i class="fa fa-check"></i><b>5.1.5</b> scGMAI</a></li>
<li class="chapter" data-level="5.1.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-1-6"><i class="fa fa-check"></i><b>5.1.6</b> scIGANs</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-2"><i class="fa fa-check"></i><b>5.2</b> Batch effect correction</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-1"><i class="fa fa-check"></i><b>5.2.1</b> BERMUDA: Batch Effect ReMoval Using Deep Autoencoders</a></li>
<li class="chapter" data-level="5.2.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-2"><i class="fa fa-check"></i><b>5.2.2</b> DESC: batch correction based on clustering</a></li>
<li class="chapter" data-level="5.2.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-2-3"><i class="fa fa-check"></i><b>5.2.3</b> iMAP: Integration of Multiple single-cell datasets by Adversarial Paired-style transfer networks</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-3"><i class="fa fa-check"></i><b>5.3</b> Dimension reduction, latent representation, clustering, and data augmentation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-1"><i class="fa fa-check"></i><b>5.3.1</b> Dimension reduction by AEs with gene-interaction constrained architecture</a></li>
<li class="chapter" data-level="5.3.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-2"><i class="fa fa-check"></i><b>5.3.2</b> Dhaka: a VAE-based dimension reduction model</a></li>
<li class="chapter" data-level="5.3.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-3"><i class="fa fa-check"></i><b>5.3.3</b> cvis: a VAE for capturing low-dimensional structures</a></li>
<li class="chapter" data-level="5.3.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.3.4</b> </a></li>
<li class="chapter" data-level="5.3.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.3.5</b> </a></li>
<li class="chapter" data-level="5.3.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.3.6</b> </a></li>
<li class="chapter" data-level="5.3.7" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.3.7</b> </a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-4"><i class="fa fa-check"></i><b>5.4</b> multi-functional</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.4.1</b> </a></li>
<li class="chapter" data-level="5.4.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.4.2</b> </a></li>
<li class="chapter" data-level="5.4.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.4.3</b> </a></li>
<li class="chapter" data-level="5.4.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.4.4</b> </a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="ch-5.html"><a href="ch-5.html#ch-5-5"><i class="fa fa-check"></i><b>5.5</b> Doublet Classification</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.5.1</b> </a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="ch-5.html"><a href="ch-5.html#ch-5-6"><i class="fa fa-check"></i><b>5.6</b> Automated Cell Type</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.6.1</b> </a></li>
<li class="chapter" data-level="5.6.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.6.2</b> </a></li>
<li class="chapter" data-level="5.6.3" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.6.3</b> </a></li>
<li class="chapter" data-level="5.6.4" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.6.4</b> </a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-5.html"><a href="ch-5.html#ch-5-7"><i class="fa fa-check"></i><b>5.7</b> Biological Function Prediction</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.7.1</b> </a></li>
<li class="chapter" data-level="5.7.2" data-path="ch-5.html"><a href="ch-5.html#ch-5-3-4"><i class="fa fa-check"></i><b>5.7.2</b> </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-6.html"><a href="ch-6.html"><i class="fa fa-check"></i><b>6</b> Conclusion and Discussions</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-5" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Survey of deep learning models for scRNA-Seq analysis</h1>
<p>In this section, we survey applications of DL models for scRNA-seq analysis. To better understand the relationship between the problems that each surveyed work addresses and the key challenges in the general scRNA-seq processing pipeline, we divide the survey into sections according to steps in the scRNA-seq processing pipeline illustrated in Fig.<a href="ch-2.html#fig:Figure1">2.1</a>. For each DL model, we present the model details under the general model framework introduced in Section <a href="ch-3.html#ch-3">3</a> and discuss the specific loss functions. We also survey the evaluation metrics and summarize the evaluation results. To facilitate cross-references of the information, we summarized all algorithms reviewed in this section in Table <a href="#tab:Table1"><strong>??</strong></a> and tabulate the datasets and evaluation metrics used in each paper in Tables <a href="#tab:Table2"><strong>??</strong></a> &amp; <a href="#tab:Table3"><strong>??</strong></a>. We also listed all other algorithms that each surveyed method evaluated against in Fig.<a href="#fig:Figure3"><strong>??</strong></a>, highlighting the extensiveness these algorithms were assessed for their performance.</p>

<div id="ch-5-1" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Imputation</h2>
<p>The goal of imputation is to estimate the missing gene expression values due to dropout, or the failure to amplify the original RNA transcripts. These missing expression values can affect downstream bioinformatics analysis significantly as it decreases the power of the studies and introduces biases in gene expression [17]. VAE, AE, and GAN have been applied for imputation and we review their specific model designs in this section.</p>

<div id="ch-5-1-1" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> DCA:deep count autoencoder</h3>
<p>DCA [15] is an AE designed for imputation (Fig.<a href="ch-3.html#fig:Figure2">3.1</a>-B. DCA is implemented in Python as a command line and also integrated into the Scanpy framework.</p>
<p><em>Model.</em> DCA models UMI counts of a cell with missing values using the ZINB distribution as</p>
<p><span class="math display" id="eq:eq11">\[\begin{equation}
p(x_{gn} \vert \Theta)=\pi_{gn}\delta(0)+(1-\pi_{gn})NB(v_{gn},\alpha_{gn}),\space for\space g=1,…G; n=1,…N \tag{5.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\delta(⋅\)</span>) is a Dirac delta function,<span class="math inline">\(NB(,)\)</span> denotes the negative binomial distribution and <span class="math inline">\(\pi_{gn},v_{gn}, \space and \space \alpha_{gn}\)</span> are dropout rate, mean, and dispersion, which are functions of DCA decoder output <span class="math inline">\(\bf{\hat{x}_{n}}\)</span> as</p>
<p><span class="math display" id="eq:eq12">\[\begin{equation}
\bf{\pi_{n}}=sigmoid(\bf{W_{\pi}\hat{x}_{n}}); \bf{v_{n}}=exp(\bf{W_{v}\hat{x}_{n}}); \bf{\alpha_{n}}=exp(\bf{W_{\alpha}\hat{x}_{n}}) \tag{5.2}
\end{equation}\]</span></p>
<p>where the <span class="math inline">\(g\)</span>th element of <span class="math inline">\(\bf{\pi_{n}}\)</span>, <span class="math inline">\(\bf{v_{n}}\)</span>, and <span class="math inline">\(\bf{\alpha_{n}}\)</span> are <span class="math inline">\(\pi_{gn}\)</span> and <span class="math inline">\(\alpha_{gn}\)</span>, respectively and <span class="math inline">\(\bf{W_{\pi}}\)</span>, <span class="math inline">\(\bf{W_{v}}\)</span>, and <span class="math inline">\(\bf{W_{\alpha}}\)</span> are additional weights to be estimated.
The DCA encoder and decoder follow the general AE formulation as in Eq. (<a href="ch-3.html#eq:eq7">(3.7)</a>) but the encoder takes the size factor normalized, log-transformed expression as input.
The encoder and decoder architecture are the conventional deep neural networks.
The parameters to be trained are <span class="math inline">\(\Theta=\{\theta,\phi,W_{\pi},W_{v},W_{\alpha}\}\)</span>.
To train the model, DCA uses a constrained log-likelihood as the loss function as</p>
<p><span class="math display" id="eq:eq13">\[\begin{equation}
L(\Theta)=\sum_{n=1}^{N} \sum_{g=1}^{G} (-logp(x_{gn}\vert \Theta) + \lambda\pi^{2}_{gn} ) \tag{5.3}
\end{equation}\]</span></p>
<p>where the second term functions as a ridge prior on the dropout probabilities.
Once the DCA is trained, the mean counts <span class="math inline">\(v_{n}\)</span> are used as the denoised and imputed counts for cell <span class="math inline">\(n\)</span>.</p>
<p><em>Evaluation metrics.</em> A Density Resampled Estimate of Mutual Information (DREMI) measure was adapted for the higher dimensionality and sparsity of scRNA-seq data.</p>
<p><em>Results.</em> For evaluation, DCA is compared to other methods using simulated data (using Splatter R package), and a real bulk transcriptomics data from a developmental C. elegans time-course experiment was used with added simulating single-cell specific noise. For this. gene expression was measured from 206 developmentally synchronized young adults over a twelve-hour period (C elegans). Single-cell specific noise was added in silico by genewise subtracting values drawn from the exponential distribution such that 80<span class="math inline">\(%\)</span> of values were zeros. The paper analyzed the Bulk contains less noise than single-cell transcriptomics data and can thus aid in evaluating single-cell denoising methods by providing a good ground truth model. The authors also did a comparison of other methods like SAVER [32], scImpute [33], and MAGIC[55]. DCA denoising recovered original time-course gene expression pattern while removing single-cell specific noise. Overall, DCA demonstrates the strongest recovery of the top 500 genes most strongly associated with development in the original data without noise; is shown to outperform other existing methods in capturing cell population structure in real data (using PBMC, CITE-seq, runtime scales linearly with the number of cells.</p>

</div>
<div id="ch-5-1-2" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> SAVER-X: single-cell analysis via expression recovery harnessing external data</h3>
<p>SAVER-X [56] is an AE model developed to denoise and impute scRNA-seq data with transfer learning from other data resources. SAVER-X is implemented in R with the support of Python package sctransfer.
Model. SAVER-X decomposes the variation in the observed counts <span class="math inline">\(x_{n}\)</span> with missing values into three components: i predictable structured component that represents the shared variation across genes, ii) unpredictable cell-level biological variation and gene-specific dispersions, and iii) technical noise. Specifically, <span class="math inline">\(x_{gn}\)</span> is modeled as a Poisson-Gamma hierarchical model,</p>
<p><span class="math display" id="eq:eq14">\[\begin{equation}
p(x_{gn}\vert \Theta)=Poisson(l_{n}x&#39;_{gn}), \space \space p(x&#39;_{gn}\vert v_{gn},\alpha_{g})=Gamma(v_{gn},\alpha_{g}v_{gn}^{2}) \tag{5.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(l_{n}\)</span> is the sequencing depth of cell <span class="math inline">\(n\)</span>, <span class="math inline">\(ν_{gn}\)</span> is the mean, and <span class="math inline">\(\alpha_{g}\)</span> is the dispersion. This Poisson-Gamma mixture is an equivalent expression to the NB distribution. As a result, the ZINB distribution as Eq. (<a href="ch-5.html#eq:eq11">(5.1)</a>) in DCA is also adopted to model missing values.
To train the model, a similar log-likelihood function as Eq. (<a href="ch-5.html#eq:eq13">(5.3)</a>) in DCA is used as the loss function. However, <span class="math inline">\(ν_{gn}\)</span> is initially learned by an AE pre-trained using external datasets from an identical or similar tissue and then transferred to and updated using data <span class="math inline">\(x_{n}\)</span> to be denoised. Such transfer learning can be applied to data between species (e.g., human and mouse in the study), cell types, batches, and single-cell profiling technologies. After <span class="math inline">\(ν_{gn}\)</span> is inferred, SAVER-X generates the final denoised data <span class="math inline">\(\hat{x}_{gn}\)</span> by an empirical Bayesian shrinkage.</p>
<p><em>Evaluation metrics.</em> <span class="math inline">\(t\)</span>-SNE visualization and ARI were used to evaluate the clustering performance after imputation. Pearson correlation coefficients between protein and corresponding gene expression levels after denoising CITE-seq data were computed.</p>
<p><em>Results.</em> SAVER-X was applied to multiple human single-cell datasets of different scenarios: i) T-cell subtypes, ii) a cell type (CD4+ tegulatory T cells) that was absent from the pretraining dataset, iii) gene-protein correlations of CITE-seq data, and iv) immune cells of primary breast cancer samples with a pretraining on normal immune cells. SAVER-X with pretraining on HCA and/or PBMCs outperformed the same model without pretraining and other denoising methods, including DCA [23], scVI[13], scImpute [33], and MAGIC [55]. The model achieved promising results even for genes with very low UMI counts. SAVER-X was also applied for a cross-species study in which the model was pretrained on a human or mouse dataset and transferred to denoise another. The results demonstrated the merit of transferring public data resources to denoise in-house scRNA-seq data even when the study species, cell types, or single-cell profiling technologies are different.</p>

</div>
<div id="ch-5-1-3" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> DeepImpute (Deep neural network Imputation)</h3>
<p>DeepImpute [17] is a deep neural network model that imputes genes in a divide-and-conquer approach. DeepImpute implemented in Keras framework/TenorFlow environment.</p>
<p><em>Model.</em> For each dataset, DeepImpute selects to impute a list of genes or highly variable genes (variance over mean ratio, default = 0.5). Each sub-neural network aims to understand the relationship between the input genes (input layer) and a subset of target genes (output layer). Genes are first divided into N random subsets of size 512 called target genes. For each subset, a neural network of four layers (input, dense, dropout and output layers) is trained where the input layer includes genes (predictor genes) who are among top 5 best correlated genes to target genes but not part of the target genes in the subset. The loss is defined as the weighted MSE</p>
<p><span class="math display" id="eq:eq15">\[\begin{equation}
\mathcal{L}_{c}=\sum x_{n}(x_{n}-\hat{x}_{n})^{2}\tag{5.5}
\end{equation}\]</span></p>
<p>This function gives higher weights to genes with higher expression values, thus emphasizing accuracy on high confidence values and avoiding over penalizing genes with extremely low values.</p>
<p><em>Evaluation metrics.</em> DeepImpute computes mean squared error (MSE) and Pearson’s correlation coefficient between imputed and true expression.</p>
<p><em>Result.</em> DeepImpute had the highest overall accuracy and offered faster computation time with less demand on computer memory compared to other methods like MAGIC, DrImpute, ScImpute, SAVER, VIPER, and DCA. Using simulated and experimental datasets (Table <a href="#tab:Table2"><strong>??</strong></a>), DeepImpute showed benefits in increasing clustering results and identifying significantly differentially expressed genes. DeepImpute and DCA, show overall advantages over other methods and between which DeepImpute performs even better. The properties of DeepImpute contribute to its superior performance include 1) a divide-and-conquer approach which contrary to an autoencoder as implemented in DCA, resulting in a lower complexity in each sub-model and stabilizing neural networks, and 2) the subnetworks are trained without using the target genes as the input which reduces overfitting while enforcing the network to understand true relationships between genes.</p>

</div>
<div id="ch-5-1-4" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> LATE: Learning with AuToEncoder</h3>
<p>LATE [57] is an AE whose encoder takes the log-transformed expression as input. LATE implemented in Python with TensorFlow.</p>
<p><em>Model. </em> LATE sets zeros for all missing values at the input and generates the imputed expressions at the decoder’s output. LATE experimented with three different network architectures composed of 1, 3 and 5 hidden layers. LATE minimizes the MSE loss as defined in Eq. (9). One problem with this model is that it assumes that all the zeros in the scRNA-seq data are missing values but some zeros could be real and reflect the actual lack of expression.</p>
<p><em>Evaluation metrics.</em> Like DeepImpute, LATE used MSE to evaluate the performance.</p>
<p><em>Result.</em> Using synthetic data generated from pre-imputed data followed with random dropout selection at different degree, LATE is shown to outperform other existing methods like MAGIC, SAVER, DCA, scVI, particularly when the ground truth contains only a few or no zeros. However, when the data contain many zero expression values, DCA achieved a lower MSE than LATE, although LATE still has a smaller MSE than scVI. This result suggests that DCA likely does a better job identifying true zeros gene expression, partly due to that LATE does not make assumptions on the statistical distributions of the single-cell data that potentially have inflated zero counts.</p>

</div>
<div id="ch-5-1-5" class="section level3" number="5.1.5">
<h3><span class="header-section-number">5.1.5</span> scGMAI</h3>
<p>Technically, scGMAI [58] is a model for clustering but it includes an AE in the first step to combat dropout. The scGAMI’s AE model is implemented with TensorFlow.</p>
<p><em>Model. </em> To impute the missing values, scGMAI applies an AE like LATE to reconstruct log-transformed expressions with dropout. One difference is that it chooses Softplus as the activation function since it is smoother than ReLU and thus more suitable for scRNA-seq data. The MSE loss as in (9) is adopted.
After imputation, scGMAI uses fast independent component analysis (ICA) on the AE reconstructed expression to reduce the dimension and then applies a Gaussian mixture model on the ICA reduced data to perform the clustering.</p>
<p><em>Evaluation metrics.</em> It used clustering metrics including NMI, ARI, Homogeneity, and Completeness to evaluate the performance.</p>
<p><em>Results.</em> To assess the performance, the AE in scGMAI was replaced by five other imputation methods including SAVER [32], MAGIC [55], DCA [23], scImpute [33], and CIDR[59]. A scGMAI implementation without AE was also compared. Seventeen scRNA-seq data (part of them are listed in Table <a href="#tab:Table2"><strong>??</strong></a> as marked) were used to evaluate cell clustering performances. The results indicated that the AEs significantly improved the clustering performance in eight of seventeen scRNA-Seq datasets.</p>

</div>
<div id="ch-5-1-6" class="section level3" number="5.1.6">
<h3><span class="header-section-number">5.1.6</span> scIGANs</h3>
<p>Imputation approaches based on information from cells with similar expressions suffer from oversmoothing, especially for rare cell types. scIGANs [16] is a GAN-based imputation algorithm, which overcomes this problem by using the observed samples with missing values to train a GAN to generate samples with imputed expressions.</p>
<p><em>Model.</em> The gene expression vector <span class="math inline">\(x_{n}\)</span> is first reshaped into a square image-like format and scIGAN takes the reshaped data as input. The model follows a BEGAN [60] framework, which substitutes the generator with an autoencoder that includes an encoder E and a decoder G and also replaces the discriminator <span class="math inline">\(D\)</span> with a function <span class="math inline">\(R_{\phi_{R}}\)</span>) that computes the reconstruction error of the autoencoder (e.g. MSE). Then, the Wasserstein distance between the reconstruction errors of the real and generated samples are computed as the loss</p>
<p><span class="math display" id="eq:eq16">\[\begin{equation}
L(\theta,\phi) = \max_{\phi_{R}}\sum_{n=1}^{N}R_{\phi_{R}}(x_{n})-\sum_{n=1}^{N}R_{\phi_{R}}(G_{\theta}(E_{\phi}(x_{n}),y) \tag{5.6}
\end{equation}\]</span></p>
<p>The encoder and decoder are trained in a GAN fashion to minimize this Wasserstein distance. This framework forces the model to meet two computing objectives, i.e. reconstructing the real samples and discriminating between real and generated samples. Proportional Control Theory was applied to balance these two goals during the training.<br />
After training, the decoder <span class="math inline">\(G_{\theta}\)</span> is used to generate new samples of a specific cell type. Then, the k-nearest neighbors (KNN) approach is applied to the real and generated samples to impute the real samples’ missing expressions.</p>
<p><em>Evaluation metrics.</em> It used a variety of clustering- and classification-based metrics including ARI, ACC, AUC and F-score.</p>
<p><em>Results.</em> scIGANs was first tested on simulated samples with different dropout rates. Performance of rescuing the correct clusters was compared with 11 existing imputation approaches including DCA, DeepImpute, SAVER, scImpute, MAGIC, etc. scIGANs reported the best performance for all metrics. scIGAN was next evaluated for correctly clustering cell types on the Human brain scRNA-seq data and showed superior performance than existing methods again. scIGANs was next evaluated for identifying cell-cycle states using scRNA-seq datasets from mouse embryonic stem cells. The results showed that scIGANs outperformed competing existing approaches for recovering subcellular states of cell cycle dynamics. scIGANs was further shown to improve the identification of differentially expressed genes and enhance the inference of cellular trajectory using time-course scRNA-seq data from the differentiation from H1 ESC to definitive endoderm cells (DEC). Finally, scIGAN was also shown to scale to scRNA-seq methods and data sizes.</p>

</div>
</div>
<div id="ch-5-2" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Batch effect correction</h2>
<p>The goal of imputation is to estimate the missing gene expression values due to dropout, or the failure to amplify the original RNA transcripts. These missing expression values can affect downstream bioinformatics analysis significantly as it decreases the power of the studies and introduces biases in gene expression [17]. VAE, AE, and GAN have been applied for imputation and we review their specific model designs in this section.</p>

<div id="ch-5-2-1" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> BERMUDA: Batch Effect ReMoval Using Deep Autoencoders</h3>
<p>BERMUDA [61] deploys a transfer-learning method to remove the batch effect. It performs correction to the shared cell clusters among batches and therefore preserves batch-specific cell populations.
Model. BERMUDA has a conventional AE architecture that takes normalized, log-transformed expression of a cell as input. It has the general loss function but consists of two parts as</p>
<p><span class="math display" id="eq:equ17">\[\begin{equation}
L(\Theta) = L_{0}(\Theta) + \lambda L_{MMD}(\Theta) \tag{5.7}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(L_{0}(\Theta)\)</span> is the MSE reconstruction loss as defined in Eq. (<a href="ch-3.html#eq:eq9">(3.9)</a>) and L_MMD is the maximum mean discrepancy (MMD) loss that measures the differences in distributions among similar cell clusters in different batches. MMD is a non-parametric distance between distributions based on the reproducing kernel Hilbert space (RKHS) [51]. Instead of applying the MMD loss on batches entirely, BERMUDA considers the loss only between pairs of similar cell clusters shared among batches, where the MMD loss is defined as:</p>
<p><span class="math display" id="eq:equ18">\[\begin{equation}
L_{MMD}(\Theta) = \sum_{i_{a},i_{b},j_{a},j_{b}}M_{i_{a},i_{b},j_{a},j_{b}}MMD(z_{i_{a},j_{a}},z_{i_{b}.j_{b}}) \tag{5.8}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(z_{i,j}\)</span> is the latent variable of <span class="math inline">\(x_{i,j}\)</span>, the input expression profile of a cell from cluster j of batch <span class="math inline">\(i\)</span>, <span class="math inline">\(M_{i_{a},j_{a},i_{b},j_{b}}\)</span> is 1 if cluster <span class="math inline">\(i_{a}\)</span> of batch <span class="math inline">\(j_{a}\)</span> and cluster <span class="math inline">\(i_{b}\)</span> of batch <span class="math inline">\(j_{b}\)</span> are determined to be similar by MetaNeighbor [62] and 0, otherwise. <span class="math inline">\(MMD()\)</span> equals zero when the underlying distributions of the observed samples are the same. By minimizing the MMD loss between the distributions of the latent variables of similar clusters, BERMUDA can be trained to remove batch effects in its latent variables.</p>
<p><em>Evaluation metrics.</em> Evaluation of BERMUDA included three metrics: KBET, entropy of mixing, and silhouette index.</p>
<p><em>Results.</em> BERMUDA was shown to outperform other methods like mnnCorrect [28], BBKNN[63], Seurat [9], and scVI [13] in removing batch effects on simulated and human pancreas data while preserving batch-specific biological signals. BERMUDA provides several improvements compared to existing methods: 1) capable of removing batch effects even when the cell population compositions across different batches are vastly different; and 2) preserving batch-specific biological signals through transfer-learning which enables discovering new information that might be hard to extract by analyzing each batch individually.</p>

</div>
<div id="ch-5-2-2" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> DESC: batch correction based on clustering</h3>
<p>DESC [64] is an AE that removes batch effect through clustering with hypothesis that batch differences in expressions are smaller than true biological variations between cell types, and, therefore, properly performing clustering on cells across multiple batches can remove batch effects without the need to define batches explicitly.</p>
<p><em>Model.</em> DESC has a conventional AE architecture. Its encoder takes normalized, log-transformed expression as the input and uses decoder output <span class="math inline">\(\hat{x}_{n}\)</span> as the reconstructed gene expression, which is equivalent to a Gaussian data distribution with <span class="math inline">\(\hat{x}_{n}\)</span> being the mean. The loss function is
<span class="math display" id="eq:eq19">\[\begin{equation}
  L(\Theta) = L_{0}(\Theta)+\gamma L_{c}(\Theta) \tag{5.9}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(L_{0}\)</span> is reconstruction MSE as defined in Eq. (<a href="ch-3.html#eq:eq8">(3.8)</a>) and <span class="math inline">\(L_{c}\)</span> is the clustering loss that regularizes the learned feature representations to form clusters. The clustering loss follows the design in the deep embedded clustering [65] . Let <span class="math inline">\(k \in \{1,…K\}\)</span> be the cluster index of a cell and assume that there are <span class="math inline">\(K\)</span> clusters. Then, the clustering loss is defined as a Kullback–Leibler (KL) divergence</p>
<p><span class="math display" id="eq:eq20">\[\begin{equation}
L_{c}(\Theta) - KL(P\|Q)  = \sum_{n=1}^{N} \sum_{k=1}^{K}p_{nk}\log{\frac{p_{nk}}{q_{nk}}} \tag{5.10}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(q_{nk}\)</span> is the probability of cell n belonging to cluster <span class="math inline">\(k\)</span> and computed as</p>
<p><span class="math display" id="eq:eq21">\[\begin{equation}
q_{nk} = \frac{ (1+\|z_{n}-\mu_{k}\|^{2})^{-1}}{\sum_{k&#39;}(1+\|z_{n}-\mu_{k&#39;}\|^{2})^{-1}} \tag{5.11}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(μ_{k}\)</span> is the center of cluster <span class="math inline">\(k\)</span> and <span class="math inline">\(p_{nk}\)</span> is the target distribution calculated by normalizing <span class="math inline">\(q_{nk}^{2}\)</span> with frequency per cluster</p>
<p><span class="math display" id="eq:eq22">\[\begin{equation}
p_{nk}=\frac{q&#39;_{nk}}{\sum_{k&#39;=1}^{K}q&#39;_{nk&#39;}}, \space and \space q&#39;_{nk}=\frac{q_{nk}^{2}}{\sum_{n=1}^{N}q_{nk}} \tag{5.12}
\end{equation}\]</span></p>
<p>A standard <span class="math inline">\(k\)</span>-means clustering algorithm is used to initialize cluster centers. The model is first trained to minimize <span class="math inline">\(L_{0}\)</span> only to obtain the initial weights before the model is trained to optimize the combined loss (19). When the training is done, each cell’s cluster index can be assigned based on <span class="math inline">\(p_{nk}\)</span>. After the training, each cell is assigned with a cluster ID.</p>
<p><em>Evaluation metrics.</em> ARI was used for clustering and the KL divergence (Eq. <a href="ch-5.html#eq:eq20">(5.10)</a>) was used to assess the batch effect removal.</p>
<p><em>Results.</em> DESC was applied to the macaque retina dataset, which includes animal level, region level, and sample-level batch effects. The results showed that DESC is effective in removing the batch effect, whereas CCA [29], MNN [28], Seurat 3.0 [9], scVI [13], BERMUDA [61], and scanorama [66] were all sensitive to batch definitions. DESC was then applied to human pancreas datasets to test its ability to remove batch effects from multiple scRNA-seq platforms and yielded the highest ARI among the comparing approaches mentioned above. When applied to human PBMC data with interferon-beta stimulation, where biological variations are compounded by batch effect, DESC was shown to be the best in removing batch effect while preserving biological variations. DESC was also shown to remove batch effect for the monocytes and mouse bone marrow data and DESC was shown to preserve the pseudotemporal structure. Finally, DESC scales linearly with the number of cells and its running time is not affected by the increasing number of batches.</p>

</div>
<div id="ch-5-2-3" class="section level3" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> iMAP: Integration of Multiple single-cell datasets by Adversarial Paired-style transfer networks</h3>
<p>iMAP [67] combines AE and GAN for batch effect removal. It is designed to remove batch biases while preserving dataset-specific biological variations.</p>
<p><em>Model.</em> iMAP consists of two processing stages, each including a separate DL model. In the first stage, a special AE, whose decoder combines the output of two separate decoders <span class="math inline">\(D_{\theta_{1}}\)</span> and $D_{_{2}}, is trained such that</p>
<p><span class="math display" id="eq:eq23">\[\begin{equation}
z_{n} = E_{\phi}; \hat{x}_{n}=D_{\theta}(z_{n},s_{n}) = ReLu(D_{\theta_{1}}(s_{n}) + D_{\theta_{2}}(z_{n}, s_{n})) \tag{5.13}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(s_{n}\)</span> is the one-hot encoded batch number of cell <span class="math inline">\(n\)</span>. $D_{θ_{1}} can be understood as decoding the batch noise, whereas $D_{θ_{2}} reconstructs batch-removed expression from the latent variable <span class="math inline">\(z_{n}\)</span>. The training minimizes the loss</p>
<p><span class="math display" id="eq:eq24">\[\begin{equation}
L(\Theta)=L_{0}(\Theta)+\gamma L_{t}(\Theta) \tag{5.14}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(L_{0}(\Theta)\)</span> is the MSE reconstruction loss defined in eq. (<a href="ch-3.html#eq:eq7">(3.7)</a>) and <span class="math inline">\(L_{c}\)</span> is the content loss</p>
<p><span class="math display" id="eq:eq25">\[\begin{equation}
L_{t}(\Theta)= \sum_{n=1}^{N} \| z_{n}-E_{\phi}(D_{\theta}(z_{n},\tilde{s}_{n})) \|_{2}^{2} \tag{5.15}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\tilde{s_{n}}\)</span> is a random batch number. Minimizing <span class="math inline">\(L_{t}(\Theta)\)</span> further ensures the reconstructed expression <span class="math inline">\(\hat{x_{n}}\)</span> would be batch agnostic and has the same content as <span class="math inline">\(x_{n}\)</span>.
However, the author indicated that due to the limitation of AE, this step is still insufficient for batch removal. Therefore, a second stage is included to apply a GAN model to make expression distributions of the shared cell type across different baches indistinguishable. To identified the shared cell types, a mutual nearest neighbors (MNN) strategy adapted from [28] was developed to identify MNN pairs across batches using batch effect independent <span class="math inline">\(z_{n}\)</span> as opposed to <span class="math inline">\(x_{n}\)</span>. Then, a mapping generator <span class="math inline">\(G_{\theta_{G}}\)</span> is trained using MNN pairs based on GAN such that <span class="math inline">\(x_{n}^{(A)} =G_{\theta_{G}}(x_{n}^{(s)})\)</span>, where <span class="math inline">\(x_{n}^{(S)}\)</span> and <span class="math inline">\(x_{n}^{(A)}\)</span> are the MNN pairs from batch <span class="math inline">\(S\)</span> and an anchor batch <span class="math inline">\(A\)</span>. The WGAN-GP loss as in Eq. (<a href="ch-3.html#eq:eq10">(3.10)</a>) was adopted for the GAN training. After training, <span class="math inline">\(G_{\theta_{G}}\)</span> is applied to all cells of a batch to generate batch-corrected expression.</p>
<p><em>Evaluation matrics.</em> The classifier-based metric as described in section <a href="#ch-4-2-2"><strong>??</strong></a> was used.</p>
<p><em>Results:</em> iMAP was first tested on benchmark datasets from human dendritic cells and Jurkat and 293T cell lines and then human pancreas datasets from five different platforms. All the datasets contain both batch-specific cells and batch-shared cell types. iMAP was shown to separate the batch-specific cell types but mix batch shared cell types and outperformed 9 other existing batch correction methods including Harmony, scVI, fastMNN, Seurat, etc. iMAP was then applied to the large-scale Tabula Muris datasets containing over 100K cells sequenced from two platforms. iMAP could not only reliably integrate cells from the same tissues but identify cells from platform-specific tissues. Finally, iMAP was applied to datasets of tumor-infiltrating immune cells and shown to reduce the dropout ratio and the percentage of ribosomal genes and noncoding RNAs, thus improving detection of rare cell types and ligand-receptor interactions. iMAP scales with the number of cells, showing minimal time cost increase after the number of cells exceeds thousands. Its performance is also robust against model hyperparameters.</p>

</div>
</div>
<div id="ch-5-3" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Dimension reduction, latent representation, clustering, and data augmentation</h2>
<p>Dimension reduction is indispensable for many type of scRNA-seq data analysis, considering the limited number of cell types in each biospecimen. Furthermore, biological processes of interests often involve the complex coordination of many genes, therefore, latent representation which capture biological variation in reduced dimentions are useful in interpreting many experiment conditions. In addition, many deep learning models further exploit latent dimentions and generative factors to produce augmented data that may in return to enhance the clustering, e.g., due to low representation of certain cell types. Therefore, we categories all these algorithms together in this section.</p>

<div id="ch-5-3-1" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Dimension reduction by AEs with gene-interaction constrained architecture</h3>
<p>This study [68] considers AEs for learning the low-dimensional representation and specifically explores the benefit of incorporating prior biological knowledge of gene-gene interactions to regularize the AE network architecture.</p>
<p><em>Model.</em> Several AE models with single or two hidden layers that incorporate gene interactions reflecting transcription factor (TF) regulations and protein-protein interactions (PPIs) are implemented. The models take normalized, log-transformed expressions and follow the general AE structure, including dimension-reducing and reconstructing layers, but the network architectures are not symmetrical. Specifically, gene interactions are incorporated such that each node of the first hidden layer represented a TF or a protein in the PPI; only genes that are targeted by TFs or involved in the PPI were connected to the node. Thus, the corresponding weights of <span class="math inline">\(E_{\phi}\)</span> and <span class="math inline">\(D_{\theta}\)</span> are set to be trainable and otherwise fixed at zero throughout the training process. Both unsupervised (AE-like) and supervised (cell-type label) learning were studied.</p>
<p><em>Evaluation metrics.</em> Performance of cells clustering was evaluated by six metrics including NMI, ARI, completeness, Fowlkes–Mallows score [69], homogeneity, and v-measure [44]. Performance of cell-type retrieval was evaluated by the mean of average precision.</p>
<p><em>Results.</em> Regularizing encoder connections with TF and PPI information considerably reduced the model complexity by almost 90% (7.5-7.6M to 1.0-1.1M). The clusters formed on the data representations learned from the models with or without TF and PPI information were compared to those from PCA, NMF, independent component analysis (ICA), t-SNE, and SIMLR [40]. The model with TF/PPI information and 2 hidden layers achieved the best performance by five of the six measures (0.87-0.92) and the best average performance (0.90). In terms of the cell-type retrieval of single cells, the encoder models with and without TF/PPI information achieved the best performance in 4 and 3 cell types, respectively. PCA yielded the best performance in only 2 cell types. The DNN model with TF/PPI information and 2 hidden layers again achieved the best average performance (mean of average precision, 0.58) across all cell types. In summary, this study demonstrated a biologically meaningful way to regularize AEs by the prior biological knowledge for learning the representation of scRNA-Seq data for cell clustering and retrieval.</p>

</div>
<div id="ch-5-3-2" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Dhaka: a VAE-based dimension reduction model</h3>
<p>Dhaka [70] was proposed to reduce the dimension of scRNA-Seq data for efficient stratification of tumor subpopulations.</p>
<p><em>Model.</em> Dhaka adopts a general VAE formulation. It takes the normalized, log-transformed expressions of a cell as input and outputs the low-dimensional representation.</p>
<p><em>Evaluation matrics.</em> ARI was used to determine the quality of the resulting clustering for each dimensionality reduction method. Spearman rank correlation was assessed to the scoring metric (lineage or differentiation) to contrast with other programs.</p>
<p><em>Result.</em> Dhaka was first tested on the simulated dataset. The simulated dataset contains 500 cells, each including 3K genes, clustered into 5 different clusters with 100 cells each. The clustering performance was compared with other methods including t-SNE, PCA, SIMLR, NMF, an autoencoder, MAGIC, and scVI. Dhaka was shown to have an ARI higher than most other comparing methods. Dhaka was then applied to the Oligodendroglioma data and could separate malignant cells from non-malignant microglia/macrophage cells. It also uncovered the shared glial lineage and differentially expressed genes for the lineages. Dhaka was also applied to the Glioblastoma data and revealed an evolutionary trajectory of the malignant cells where cells were gradually progressing from a stemlike state to a more differentiated state. In contrast, other methods failed to capture this underlying structure. Dhaka was next applied to the Melanoma cancer dataset [71] and uncovered two distinct clusters that showed the intra-tumor heterogeneity of the Melanoma samples. Dhaka was finally applied to copy number variation data [72] and shown to identify one major and one minor cell clusters, of which other methods could not find.</p>

</div>
<div id="ch-5-3-3" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> cvis: a VAE for capturing low-dimensional structures</h3>
<p>scvis [73] is a VAE network that learns the low-dimensional representations capture both local and global neighboring structures in scRNA-Seq data.</p>
<p><em>Model:</em> scvis adopts the generic VAE formulation described in section <a href="ch-3.html#ch-3-1">3.1</a> However, it has a unique loss function defined as</p>
<p><span class="math display" id="eq:eq26">\[\begin{equation}
L(\Theta)= - \mathcal{L}(\Theta) + \lambda L_{t}(\Theta) \tag{5.16}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(L(\Theta)\)</span> is ELBO as in Eq. (<a href="ch-3.html#eq:eq3">(3.3)</a>) and <span class="math inline">\(L_{t}\)</span> is a regularizer using non-symmetrized t-SNE objective function [73], which is defined as</p>
<p><span class="math display" id="eq:eq27">\[\begin{equation}
L_{t}(\Theta)= \sum_{i=1}^{N}\sum_{j=1,j\neq i}^{N} p_{j\vert i}\log{\frac{p_{j \vert i}}{q_{j \vert i}} } \tag{5.17}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are two different cells, <span class="math inline">\(p_{i\vert j}\)</span> measures the local cell relationship in the data space, and <span class="math inline">\(q_{j \vert i}\)</span> measures such relationship in the latent space as</p>
<p><span class="math display" id="eq:eq28">\[\begin{equation}
p_{i \vert j} = \frac{exp(- \frac{\|x_{i}-x_{j} \|^{2}}{2\sigma_{i}^{2}})}{\sum_{k\ne i}exp(- \frac{\|x_{i}-x_{k} \|^{2}}{2\sigma_{i}^{2}})},\space q_{j \vert i} = \frac{(1+\|z_{i}-z_{j} \|^{2})^{-1}}{\sum_{k\ne i}(1+\|z_{i}-z_{k} \|^{2})^{-1}} \tag{5.18}
\end{equation}\]</span></p>
<p>with <span class="math inline">\(\sigma_{i}\)</span> defined as the perplexity <span class="citation">(<a href="#ref-RN147" role="doc-biblioref">Maaten 2008</a>)</span>. Because t-SNE algorithm preserves the local structure of high dimensional space after projecting to the lower dimension, <span class="math inline">\(L_{t}\)</span> promotes the learning of local structures of cells.</p>
<p><em>Evaluation matrics.</em> KNN preservation and log-likelihood of low dimensional mapping are used to evaluate model performance.</p>
<p><em>Results.</em> scvis was tested on the simulated data and outperformed t-SNE in a nine-dimensional space task. scvis preserved both local structure and global structure. The relative positions of all clusters were well kept but outliers were scattered around clusters. Using simulated data and comparing to t-SNE, scvis generally produced consistent and better patterns among different runs while t-SNE could not. scvis also presented good results on adding new data to an existing embedding, with median accuracy on new data at 98.1% for K= 5 and 94.8% for K= 65, when train K cluster on original data then test the classifier on new generated sample points. scvis was subsequently tested on four real datasets including metastatic melanoma, oligodendroglioma, mouse bipolar and mouse retina datasets. In each dataset, scvis was showed to preserve both the global and local structure of the data.</p>

</div>
<div id="ch-5-3-4" class="section level3" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.3.5">
<h3><span class="header-section-number">5.3.5</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.3.6">
<h3><span class="header-section-number">5.3.6</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.3.7">
<h3><span class="header-section-number">5.3.7</span> </h3>

</div>
</div>
<div id="ch-5-4" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> multi-functional</h2>

<div id="ch-5-3-4" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.4.4">
<h3><span class="header-section-number">5.4.4</span> </h3>

</div>
</div>
<div id="ch-5-5" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Doublet Classification</h2>

<div id="ch-5-3-4" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> </h3>

</div>
</div>
<div id="ch-5-6" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Automated Cell Type</h2>

<div id="ch-5-3-4" class="section level3" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.6.3">
<h3><span class="header-section-number">5.6.3</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.6.4">
<h3><span class="header-section-number">5.6.4</span> </h3>

</div>
</div>
<div id="ch-5-7" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Biological Function Prediction</h2>

<div id="ch-5-3-4" class="section level3" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> </h3>

</div>
<div id="ch-5-3-4" class="section level3" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> </h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-RN147" class="csl-entry">
Maaten, G. van der, L. &amp; Hinton. 2008. <span>“Visualizing Data Using t-SNE.”</span> Journal Article. <em>J. Mach. Learn</em> 9: 2579–2605.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Huang-AI4Medicine-Lab/survey-of-DL-for-scRNA-seq-analysis/edit/main/05-survey-of-dl-models-for-scRNA-seq-analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["survey-of-DL-for-scRNA-seq-analysis.pdf", "survey-of-DL-for-scRNA-seq-analysis.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
