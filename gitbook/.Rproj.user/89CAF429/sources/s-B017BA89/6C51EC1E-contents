
### SAUCIE {#ch-5-4-3}

SAUCIE [14] is an AE designed to perform multiple functions, including clustering, batch correlation, imputation, and visualization. SAUCIE is applied to the normalized data instead of count data.  

*Model.* SAUCIE includes multiple model components designed for different functions.
	Clustering: SAUCIE first introduced a "digital" binary encoding layer h^c∈{0,1}^J in the decoder D that functions to encode the cluster ID. To learn this encoding, an entropy loss is introduced   
	L_D=∑_(k=1)^K▒〖p_k  log⁡〖p_k 〗 〗	(37)
where p_k is the probability (proportion) of activation on neuron k by the previous layer. Minimizing this entropy loss promotes sparse neurons, thus forcing a binary encoding. To encourage clustering behavior, SAUCIE also introduced an intracluster loss as
	L_C=∑_(i,j:h_i^c=h_j^c)▒‖x ̂_i-x ̂_j ‖^2 	(38)
 which computes the distance L_C between the expressions of a pair of cells (x ̂_i,x ̂_j) that have the same cluster ID (h_i^c=h_j^c). 

	Batch correction: To correct the batch effect, an MMD loss is introduced to measure the differences in terms of the distribution between batches in the latent space
	L_B=∑_(l=1,l≠ref)^B▒〖MMD(z_ref,〗 z_l)
(39)
where B is the total number of batches and z_ref is the latent variable of an arbitrarily chosen reference batch. 
	Imputation and visualization: The output of the decoder is taken by SAUCIE as an imputed version of the input gene expression. To visualize the data without performing an additional dimension reduction directly, the dimension of the latent variable z_n is forced to 2.    
Training the model includes two sequential runs. In the first run, an autoencoder is trained to minimize the loss L_0+〖λ_B L〗_B with L_0 being the MSE reconstruction loss defined in (9) so that a batch-corrected, imputed input x ̃ can be obtained at the output of the decoder. In the second run, the bottleneck layer of the encoder from the first run is replaced by a 2-D latent code for visualization and a digital encoding layer is also introduced. This model takes the cleaned x ̃ as the input and is trained for clustering by minimizing the loss L_0+〖λ_D L〗_D+λ_C L_C. After the model is trained, x ̃ is the imputed, batch-corrected gene expression. The 2-D latent code is used for visualization and the binary encoder encodes the cluster ID. 

*Evaluation metrics.*  For clustering, the Silhouette index was used. For batch correction, a mixing score similar in spirit to KBET is introduced to assess how well samples from two batches mix. For visualization, the precision and recall of the consistency of the neighboring cells of each cell between the original data space and the SAUCIE embedding space were calculated. For imputation, in addition to visual comprision of the imputed results, R2 statistics of the imputation results on a synthetic data were reported. 

*Results.* SAUCIE was evaluated for clustering, batch correction, imputation, and visualization on both simulated and real scRNA-seq and scCyToF datasets. The performance was compared to minibatch kmeans, Phenograph [88] and 22 for clustering; MNN [28] and canonical correlation analysis (CCA) [29] for batch correction; PCA, Monocle2 [89], diffusion maps, UMAP [90], tSNE [74] and PHATE [91] for visualization; MAGIC [55], scImpute [33]  and nearest neighbors completion (NN completion) for imputation. Results showed that SAUCIE had a better or comparable performance with other approaches. Also, SAUCIE has better scalability and faster runtimes than any of the other models. SAUCIE's results on the scCyToF dengue dataset were further analyzed in greater detail. SAUCIE was able to identify subtypes of the T cell populations and demonstrated distinct cell manifold between acute and healthy subjects.    
