
## Generative adversarial networks (GANs) for scRNA-seq data {#ch-3-3}

GANs have been used for imputation, data generation and augmentation of the scRNA-seq analysis. Without loss of generality, the GAN, when applied to scRNA-seq, is designed to learn to generate gene expression profiles from $p_{x}$, the distribution of $x_{n}$, the count or normalized expression vectors of the scRNA data. The vanilla GAN consists of two deep neural networks [2]. The first network is the generator $G_{\theta}(z_{n},y_{n})$ with parameter $\theta$, which is essentially a decoder that takes a noise vector $z_{n}$ from the distribution $p_{z}$ and a class label $y$ (e.g. cell type) as input and is trained to generate $x_{f}$, a "fake" sample of a gene expression profile (Fig.\@ref(fig:Figure2)-C). Note that including a class label $y_{n}$ at the input is optional and when it is included, the model is known as the conditional GAN. The second network is the discriminator network $D_{\phi_{D}}$ with parameters $\phi_{D}$, which is a classifier trained to distinguish between the "real" sample $x$ and fake data $x_{f}$ (Fig.\@ref(fig:Figure1)-C). The generator $G_{\theta}$ and discriminator $D_{\phi_{D}}$ are trained to outplay each other, resulting in a minimax game, in which $G_{\theta}$ is forced by $D_{\phi_{D}}$ to produce better samples, which, when converge, can fool the discriminator $D_{\phi_{D}}$, thus becoming samples from $p_{x}$. The vanilla GAN suffers heavily from training instability and mode collapsing [44]. To that end, Wasserstein GAN (WGAN) was developed to effectively improve the training stability and convergence [45]. The WGAN loss computes the Wasserstein distance, also called the earth moving distance between the real and fake sample distributions [46]:  

\begin{equation}
  L(\Theta)=\max_{\phi_{D}}\sum_{n=1}^{N}D_{\phi_{D}}(x_{n})-\sum{n=1}^{N}D_{\phi_{D}}(G_{\theta}(z_{n},y_{n})) (\#eq:eq10)
\end{equation}


Additional terms can also be added to (\@ref(eq:eq10)) to constrain the functions of the generator. Training based on the WGAN loss in Eq. (\@ref(eq:eq10) ) amounts to a min-max optimization, which iterates between the discriminator and generator, where each optimization is achieved by a stochastic gradient descent algorithm through backpropagation. The WGAN requires D to be K-Lipschitz continuous [44], which can be satisfied by adding the gradient penalty to the WGAN loss [47]. Once the training is done, the generator $G_{\phi_{G}}$ can be used to generate gene expression profiles of new cells.
