
## Variational Autoencoder (VAEs) for scRNA-Seq data {#ch-3-1}


Let $x_{n}$ represent a $G \times 1$ vector of gene expression (UMI counts or normalized, log-transformed expression) of $G$ genes in cell n, where $x_{gn}$ denotes gene $g$’s expression,  which is assumed to follow some distribution $p(x_{gn} \vert v_{gn}, \alpha_{gn} )$ (e.g., zero-inflated negative binomial (ZINB) or Gaussian), where $v_{gn}$ and $\alpha_{gn}$ are parameters of the distribution (e.g., mean, variance, or dispersion) (Fig.2A). We consider the first parameter $v_{gn}$ to be of particular interest (e.g., the mean counts) for the scRNA-seq analysis and is thus further modeled as a function of a d-dimension latent variable $z_{n} \in R^{d}$ and an observed variable $s_{n}$ (e.g., the batch ID) by a decoder neural network $D_{\theta}$ (Fig.2A) as

\begin{equation}
v_{n} = D_{\theta}(z_{n}, s_{n}) (\#eq:eq1)
\end{equation}



where the $g$th element of $v_{n}$ is $v_{gn}$ and $\theta$ is a vector of decoder weights, $z_{n}$ represents a low-dimensional latent representation of gene expression and is used in all the works for visualization and clustering.  For VAE, $z_{n}$ is commonly assumed to follow a multivariate standard normal prior, i.e., $p(z_{n})=N(0,I_{d})$ with $I_{d}$ being a $d \times d$ identity matrix. The second parameter $\alpha_{gn}$ of $p(x_{gn} \vert v_{gn}, \alpha_{gn})$ is a nuisance parameter, which is assumed with a prior distribution $p(\alpha_{gn})$ and can be either estimated or marginalized in variational inference. Now define $\Theta=\{\theta, \alpha_{ng}\forall{n},g\}$ as the collection of the unknown model parameters. Then, $p(x_{gn} \vert v_{gn}, \alpha_{gn})$ and (\@ref(eq:eq1)) together define the likelihood $p(x_{gn} \vert z_{n}, s_{gn}, \Theta)$.

The goal of training or inference is to compute the maximum likelihood estimate of \Theta 

\begin{equation}
    \hat{\Theta}_{ML} = argmax_{\Theta}\sum_{n=1}^{N}\log(x_{n} \vert s_{n},\Theta) \approx argmax_{\Theta}\sum_{n=1}^{N}\textit{L}(\Theta) (\#eq:eq2)
\end{equation}

where $p(x_{n}\vert s_{n},\Theta)=\int(p(x_{n} \vert z_{n}, s_{n}, \Theta)p(z_{n})\textit{d}z_{n}$ is the marginal likelihood, which is in general analytically intractable but can be lower-bounded by the evidence lower bound (ELBO) $\textit{L}(\Theta)$, expressed as

\begin{equation}
  \textit{L}(\Theta) = E_{q(z_{n} \vert x_{n},s_{n}, \Theta)}[\log{p(x_{n}\vert z_{n},s_{n}, \Theta)}] - D_{KL}[q(z_{n}\vert x_{n},s_{n},\Theta)\|p(z_{n})] (\#eq:eq3)
\end{equation}


where $q(z_{n}│x_{n},s_{n})$ is an approximate to the intractable posterior distribution $p(z_{n}\vert x_{n},s_{n})$. To make the variational inference tractable $q(z_{n}\vert x_{n},s_{n})$ is assumed in most cases as a multivariate Gaussian  

\begin{equation}
  q(z_{n} \vert x_{n},s_{n})= N(\mu_{z_{n}},diag({\sigma_{Z_{n}}}^2)) (\#eq:eq4)
\end{equation}


whose means and variances ${\mu_{z_{n}},{\sigma_{Z_{n}}}^2}$ are given by an encoder network $E_{\phi}$ applied to $x_{n}$ and $s_{n}$ (Fig.\@ref(fig:Figure2)-A) as


\begin{equation}
   {\mu_{z_{n}},{\sigma_{Z_{n}}}^2} = E_{\phi}(x_{n},s_{n}) (\#eq:eq5)
\end{equation}


where \pi is the vector of the unknown decoder weights. Because of the approximation by $\textit{L}(\Theta)$ in (\@ref(eq:eq2)) and the introduction of the decoder network in eq.\@ref(eq:eq4), the model parameters to be estimated become $\theta=\{ \theta,\phi, \alpha_{ng}, \forall{n}, g \}$. Optimization of $\textit{L}(\Theta)$ in (\@ref(eq:eq2)) is computed efficiently by stochastic optimization, where the gradient is calculated by backpropagation.


All the surveyed papers that deploy VAE follow this general modeling process. However, an alternative and more general formulation treats it as optimization with a loss function expressed as 

\begin{equation}
L(\Theta) = - L(\Theta) + \sum_{k=1}^{K}\lambda_{k}L_{k}(\Theta)(\#eq:eq6)
\end{equation}


where  $L_{k}\forall{k}=1,…,K$ are $K$ additional function-specific losses introduced to constrain the model for different functions (clustering, cell type prediction, etc), and $\lambda_{k}$s are the Lagrange multipliers. With this general formulation, for each paper, we focus on the survey the specific choice of data distribution $p(x_{gn} \vert v_{gn},\alpha_{gn})$ that defines $L(\Theta)$, different $L_{k}$ designed for specific functions, and how the decoder and encoder are applied to model different aspects of scRNA-seq data.








<!-- See Figure \@ref(fig:cars-plot). -->

<!-- ```{r cars-plot, fig.cap="The cars data.", echo=FALSE} -->
<!-- par(mar = c(4, 4, .2, .1)) -->
<!-- plot(cars)  # a scatterplot -->
<!-- ``` -->

<!-- Also see Equation \@ref(eq:mean). -->

<!-- \begin{equation} -->
<!-- \bar{X} = \frac{\sum_{i=1}^n X_i}{n} (\#eq:mean) -->
<!-- \end{equation} -->

<!-- And see Table \@ref(tab:mtcars). -->

<!-- ```{r mtcars, echo=FALSE} -->
<!-- knitr::kable(mtcars[1:5, 1:5], caption = "The mtcars data.") -->
<!-- ``` -->






