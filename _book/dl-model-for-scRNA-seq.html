<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Overview of common deep learning models for scRNA-seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis</title>
  <meta name="description" content="3 Overview of common deep learning models for scRNA-seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Overview of common deep learning models for scRNA-seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Huang-AI4Medicine-Lab/survey-of-DL-for-scRNA-seq-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Overview of common deep learning models for scRNA-seq analysis | Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis" />
  
  
  

<meta name="author" content="Mario Flores1§, Zhentao Liu1, Tinghe Zhang1, Md Musaddaqui Hasib1, Yu-Chiao Chiu2, Zhenqing Ye2,3, Karla Paniagua1, Sumin Jo1, Jianqiu Zhang1, Shou-Jiang Gao4,6, Yufang Jin1, Yidong Chen2,3§, and Yufei Huang5,6§" />


<meta name="date" content="2021-11-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="scRNA-seq-pipeline.html"/>
<link rel="next" href="DatasetEvalMetric.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Survey of Deep Learning for scRNA-seq Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the authors</a></li>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html"><i class="fa fa-check"></i>About this book</a>
<ul>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="about-this-book.html"><a href="about-this-book.html#key-points"><i class="fa fa-check"></i>Key Points</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scRNA-seq-pipeline.html"><a href="scRNA-seq-pipeline.html"><i class="fa fa-check"></i><b>2</b> Overview of scRNA-seq processing pipeline</a></li>
<li class="chapter" data-level="3" data-path="dl-model-for-scRNA-seq.html"><a href="dl-model-for-scRNA-seq.html"><i class="fa fa-check"></i><b>3</b> Overview of common deep learning models for scRNA-seq analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dl-model-for-scRNA-seq.html"><a href="dl-model-for-scRNA-seq.html#vae"><i class="fa fa-check"></i><b>3.1</b> Variational Autoencoder</a></li>
<li class="chapter" data-level="3.2" data-path="dl-model-for-scRNA-seq.html"><a href="dl-model-for-scRNA-seq.html#ae"><i class="fa fa-check"></i><b>3.2</b> Autoencoders (AEs) for scRNA-seq data</a></li>
<li class="chapter" data-level="3.3" data-path="dl-model-for-scRNA-seq.html"><a href="dl-model-for-scRNA-seq.html#GANs"><i class="fa fa-check"></i><b>3.3</b> Generative adversarial networks (GANs) for scRNA-seq data</a></li>
<li class="chapter" data-level="3.4" data-path="dl-model-for-scRNA-seq.html"><a href="dl-model-for-scRNA-seq.html#supervised"><i class="fa fa-check"></i><b>3.4</b> Supervised deep learning models</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="DatasetEvalMetric.html"><a href="DatasetEvalMetric.html"><i class="fa fa-check"></i><b>4</b> Overview of datasets and evaluation metrics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="DatasetEvalMetric.html"><a href="DatasetEvalMetric.html#EvalMethod"><i class="fa fa-check"></i><b>4.1</b> Evaluation methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="DatasetEvalMetric.html"><a href="DatasetEvalMetric.html#imputation"><i class="fa fa-check"></i><b>4.1.1</b> Imputation</a></li>
<li class="chapter" data-level="4.1.2" data-path="DatasetEvalMetric.html"><a href="DatasetEvalMetric.html#batch-effect-correction"><i class="fa fa-check"></i><b>4.1.2</b> Batch effect correction</a></li>
<li class="chapter" data-level="4.1.3" data-path="DatasetEvalMetric.html"><a href="DatasetEvalMetric.html#clustering"><i class="fa fa-check"></i><b>4.1.3</b> Clustering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Survey.html"><a href="Survey.html"><i class="fa fa-check"></i><b>5</b> Survey of deep learning models for scRNA-Seq analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Survey.html"><a href="Survey.html#imputation-2"><i class="fa fa-check"></i><b>5.1</b> Imputation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="Survey.html"><a href="Survey.html#dca"><i class="fa fa-check"></i><b>5.1.1</b> DCA:deep count autoencoder</a></li>
<li class="chapter" data-level="5.1.2" data-path="Survey.html"><a href="Survey.html#saver-x"><i class="fa fa-check"></i><b>5.1.2</b> SAVER-X: single-cell analysis via expression recovery harnessing external data</a></li>
<li class="chapter" data-level="5.1.3" data-path="Survey.html"><a href="Survey.html#deepimpute"><i class="fa fa-check"></i><b>5.1.3</b> DeepImpute (Deep neural network Imputation)</a></li>
<li class="chapter" data-level="5.1.4" data-path="Survey.html"><a href="Survey.html#late"><i class="fa fa-check"></i><b>5.1.4</b> LATE: Learning with AuToEncoder</a></li>
<li class="chapter" data-level="5.1.5" data-path="Survey.html"><a href="Survey.html#scGMAI"><i class="fa fa-check"></i><b>5.1.5</b> scGMAI</a></li>
<li class="chapter" data-level="5.1.6" data-path="Survey.html"><a href="Survey.html#scIGANs"><i class="fa fa-check"></i><b>5.1.6</b> scIGANs</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="Survey.html"><a href="Survey.html#batch-effect-correction-2"><i class="fa fa-check"></i><b>5.2</b> Batch effect correction</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="Survey.html"><a href="Survey.html#bermuda"><i class="fa fa-check"></i><b>5.2.1</b> BERMUDA: Batch Effect ReMoval Using Deep Autoencoders</a></li>
<li class="chapter" data-level="5.2.2" data-path="Survey.html"><a href="Survey.html#desc"><i class="fa fa-check"></i><b>5.2.2</b> DESC: batch correction based on clustering</a></li>
<li class="chapter" data-level="5.2.3" data-path="Survey.html"><a href="Survey.html#imap"><i class="fa fa-check"></i><b>5.2.3</b> iMAP: Integration of Multiple single-cell datasets by Adversarial Paired-style transfer networks</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="Survey.html"><a href="Survey.html#dimension-reduction"><i class="fa fa-check"></i><b>5.3</b> Dimension reduction, latent representation, clustering, and data augmentation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="Survey.html"><a href="Survey.html#dimension-reduction-by-aes"><i class="fa fa-check"></i><b>5.3.1</b> Dimension reduction by AEs with gene-interaction constrained architecture</a></li>
<li class="chapter" data-level="5.3.2" data-path="Survey.html"><a href="Survey.html#dhaka"><i class="fa fa-check"></i><b>5.3.2</b> Dhaka: a VAE-based dimension reduction model</a></li>
<li class="chapter" data-level="5.3.3" data-path="Survey.html"><a href="Survey.html#scvis"><i class="fa fa-check"></i><b>5.3.3</b> scvis: a VAE for capturing low-dimensional structures</a></li>
<li class="chapter" data-level="5.3.4" data-path="Survey.html"><a href="Survey.html#scVAE"><i class="fa fa-check"></i><b>5.3.4</b> scVAE: VAE for single-cell gene expression data</a></li>
<li class="chapter" data-level="5.3.5" data-path="Survey.html"><a href="Survey.html#VASC"><i class="fa fa-check"></i><b>5.3.5</b> VASC: VAE for scRNA-seq</a></li>
<li class="chapter" data-level="5.3.6" data-path="Survey.html"><a href="Survey.html#scDeepCluster"><i class="fa fa-check"></i><b>5.3.6</b> scDeepCluster</a></li>
<li class="chapter" data-level="5.3.7" data-path="Survey.html"><a href="Survey.html#scsGAN"><i class="fa fa-check"></i><b>5.3.7</b> cscGAN: Conditional single-cell generative adversarial neural networks</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="Survey.html"><a href="Survey.html#multi-functional"><i class="fa fa-check"></i><b>5.4</b> Multi-functional models</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="Survey.html"><a href="Survey.html#scVI"><i class="fa fa-check"></i><b>5.4.1</b> scVI: single-cell variational inference</a></li>
<li class="chapter" data-level="5.4.2" data-path="Survey.html"><a href="Survey.html#ldvae"><i class="fa fa-check"></i><b>5.4.2</b> LDVAE: linearly decoded variational autoencoder</a></li>
<li class="chapter" data-level="5.4.3" data-path="Survey.html"><a href="Survey.html#saucie"><i class="fa fa-check"></i><b>5.4.3</b> SAUCIE</a></li>
<li class="chapter" data-level="5.4.4" data-path="Survey.html"><a href="Survey.html#scScope"><i class="fa fa-check"></i><b>5.4.4</b> scScope</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="Survey.html"><a href="Survey.html#doubletClassification"><i class="fa fa-check"></i><b>5.5</b> Doublet Classification</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="Survey.html"><a href="Survey.html#solo"><i class="fa fa-check"></i><b>5.5.1</b> Solo</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="Survey.html"><a href="Survey.html#automatedCellType"><i class="fa fa-check"></i><b>5.6</b> Automated cell type identification</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="Survey.html"><a href="Survey.html#digitalDLSorter"><i class="fa fa-check"></i><b>5.6.1</b> DigitalDLSorter</a></li>
<li class="chapter" data-level="5.6.2" data-path="Survey.html"><a href="Survey.html#scCapsNet"><i class="fa fa-check"></i><b>5.6.2</b> scCapsNet</a></li>
<li class="chapter" data-level="5.6.3" data-path="Survey.html"><a href="Survey.html#netAE"><i class="fa fa-check"></i><b>5.6.3</b> netAE: network-enhanced autoencoder</a></li>
<li class="chapter" data-level="5.6.4" data-path="Survey.html"><a href="Survey.html#scDGN"><i class="fa fa-check"></i><b>5.6.4</b> scDGN - supervised adversarial alignment of single-cell RNA-seq data</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="Survey.html"><a href="Survey.html#biologicalFunctionPrediction"><i class="fa fa-check"></i><b>5.7</b> Biological function prediction</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="Survey.html"><a href="Survey.html#CNNC"><i class="fa fa-check"></i><b>5.7.1</b> CNNC: convolutional neural network for coexpression</a></li>
<li class="chapter" data-level="5.7.2" data-path="Survey.html"><a href="Survey.html#scGen"><i class="fa fa-check"></i><b>5.7.2</b> scGen, a generative model to predict perturbation response of single cells across cell types</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Table.html"><a href="Table.html"><i class="fa fa-check"></i><b>6</b> Table</a>
<ul>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#table1"><i class="fa fa-check"></i>Table1</a>
<ul>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#a.-deep-learning-algorithms-reviewed-in-the-paper"><i class="fa fa-check"></i>A. Deep Learning algorithms reviewed in the paper</a></li>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#b.-comparison-of-deep-learning-algorithms-reviewed-in-the-paper"><i class="fa fa-check"></i>B. Comparison of Deep Learning algorithms reviewed in the paper</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#table2"><i class="fa fa-check"></i>Table2</a>
<ul>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#a.-simulated-single-cell-dataalgorithms"><i class="fa fa-check"></i>A. Simulated single-cell data/algorithms</a></li>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#b.-human-single-cell-data-sources-used-by-different-dl-algorithms"><i class="fa fa-check"></i>B. Human single-cell data sources used by different DL algorithms</a></li>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#c.-mouse-single-cell-data-sources-used-by-different-dl-algorithms"><i class="fa fa-check"></i>C. Mouse single-cell data sources used by different DL algorithms</a></li>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#d.-single-cell-data-derived-from-other-species"><i class="fa fa-check"></i>D. Single-cell data derived from other species</a></li>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#e.-large-single-cell-data-source-used-by-various-algorithms"><i class="fa fa-check"></i>E. Large single-cell data source used by various algorithms</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#table3"><i class="fa fa-check"></i>Table3</a>
<ul>
<li class="chapter" data-level="" data-path="Table.html"><a href="Table.html#evaluation-metrics-used-in-surveyed-dl-algorithms"><i class="fa fa-check"></i>Evaluation metrics used in surveyed DL algorithms</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep learning tackles single-cell analysis - A survey of deep learning for scRNA-seq analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dl-model-for-scRNA-seq" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Overview of common deep learning models for scRNA-seq analysis</h1>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure2"></span>
<img src="Figures/Figure2.png" alt="Graphical models of the surveyed DL models including A) Variational Autoencoder (VAE); B) Autoencoder (AE); and C) Generative Adversarial Network (GAN)" width="718" />
<p class="caption">
Figure 3.1: Graphical models of the surveyed DL models including A) Variational Autoencoder (VAE); B) Autoencoder (AE); and C) Generative Adversarial Network (GAN)
</p>
</div>
<p>We start our review by introducing the general formulations of widely used deep learning models. As most of the tasks including batch correction, dimensionality reduction, imputation, and clustering are unsupervised learning tasks, we will give special attention to unsupervised models including variational autoencoder (VAE), the autoencoder (AE), or generative adversarial networks (GAN). We will also discuss the general supervised and transfer learning formulations, which find their applications in cell type predictions and functional studies. We will discuss these models in the context of scRNA-seq, detailing the different features and training strategies of each model and bringing attention to their uniqueness.</p>

<div id="vae" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Variational Autoencoder</h2>
<p>Let <span class="math inline">\(x_{n}\)</span> represent a <span class="math inline">\(G \times 1\)</span> vector of gene expression (UMI counts or normalized, log-transformed expression) of <span class="math inline">\(G\)</span> genes in cell n, where <span class="math inline">\(x_{gn}\)</span> denotes gene <span class="math inline">\(g\)</span>’s expression, which is assumed to follow some distribution <span class="math inline">\(p(x_{gn} \vert v_{gn}, \alpha_{gn} )\)</span> (e.g., zero-inflated negative binomial (ZINB) or Gaussian), where <span class="math inline">\(v_{gn}\)</span> and <span class="math inline">\(\alpha_{gn}\)</span> are parameters of the distribution (e.g., mean, variance, or dispersion) (Fig.<a href="dl-model-for-scRNA-seq.html#fig:Figure2">3.1</a>A). We consider the first parameter <span class="math inline">\(v_{gn}\)</span> to be of particular interest (e.g., the mean counts) for the scRNA-seq analysis and is thus further modeled as a function of a d-dimension latent variable <span class="math inline">\(z_{n} \in R^{d}\)</span> and an observed variable <span class="math inline">\(s_{n}\)</span> (e.g., the batch ID) by a decoder neural network <span class="math inline">\(D_{\theta}\)</span> (Fig.<a href="dl-model-for-scRNA-seq.html#fig:Figure2">3.1</a>A) as</p>
<p><span class="math display" id="eq:eq1">\[\begin{equation}
v_{n} = D_{\theta}(z_{n}, s_{n}) \tag{3.1}
\end{equation}\]</span></p>
<p>where the <span class="math inline">\(g\)</span>th element of <span class="math inline">\(v_{n}\)</span> is <span class="math inline">\(v_{gn}\)</span> and <span class="math inline">\(\theta\)</span> is a vector of decoder weights, <span class="math inline">\(z_{n} \in \mathbb{R}^{d}\)</span> represents a latent representation of gene expression and is used for visualization and clustering and <span class="math inline">\(s_{n}\)</span> is an observed variable (e.g., the batch ID). For VAE, <span class="math inline">\(z_{n}\)</span> is commonly assumed to follow a multivariate standard normal prior, i.e., <span class="math inline">\(p(z_{n})=N(0,I_{d})\)</span> with <span class="math inline">\(I_{d}\)</span> being a <span class="math inline">\(d \times d\)</span> identity matrix. Further,<span class="math inline">\(\alpha_{gn}\)</span> of <span class="math inline">\(p(x_{gn} \vert v_{gn}, \alpha_{gn})\)</span> is a nuisance parameter, which has a prior distribution <span class="math inline">\(p(\alpha_{gn})\)</span> and can be either estimated or marginalized in variational inference. Now define <span class="math inline">\(\Theta=\{\theta, \alpha_{ng}\forall{n},g\}\)</span> as the collection of the unknown model parameters. Then, <span class="math inline">\(p(x_{gn} \vert v_{gn}, \alpha_{gn})\)</span> and (<a href="dl-model-for-scRNA-seq.html#eq:eq1">(3.1)</a>) together define the likelihood <span class="math inline">\(p(x_{gn} \vert z_{n}, s_{gn}, \Theta)\)</span>.</p>
<p>The goal of training or inference is to compute the maximum likelihood estimate of </p>
<p><span class="math display" id="eq:eq2">\[\begin{equation}
    \hat{\Theta}_{ML} = argmax_{\Theta}\sum_{n=1}^{N}\log(x_{n} \vert s_{n},\Theta) \approx argmax_{\Theta}\sum_{n=1}^{N}\textit{L}(\Theta) \tag{3.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\textit{L}(\Theta)\)</span> is the evidence lower bound (ELBO),</p>
<p><span class="math display" id="eq:eq3">\[\begin{equation}
  \textit{L}(\Theta) = E_{q(z_{n} \vert x_{n},s_{n}, \Theta)}[\log{p(x_{n}\vert z_{n},s_{n}, \Theta)}] - D_{KL}[q(z_{n}\vert x_{n},s_{n},\Theta)\|p(z_{n})] \tag{3.3}
\end{equation}\]</span></p>
<p>and <span class="math inline">\(q(z_{n}│x_{n},s_{n})\)</span> is an approximate to <span class="math inline">\(p(z_{n}│x_{n},s_{n})\)</span> and assumed as</p>
<p><span class="math display" id="eq:eq4">\[\begin{equation}
  q(z_{n} \vert x_{n},s_{n})= N(\mu_{z_{n}},diag({\sigma_{Z_{n}}}^2)) \tag{3.4}
\end{equation}\]</span></p>
<p>with <span class="math inline">\({\mu_{z_{n}},{\sigma_{Z_{n}}}^2}\)</span> given by an encoder network <span class="math inline">\(E_{\phi}\)</span> (Fig.<a href="dl-model-for-scRNA-seq.html#fig:Figure2">3.1</a>A) as</p>
<p><span class="math display" id="eq:eq5">\[\begin{equation}
   {\mu_{z_{n}},{\sigma_{Z_{n}}}^2} = E_{\phi}(x_{n},s_{n}) \tag{3.5}
\end{equation}\]</span></p>
<p>where is weights vector. Now, <span class="math inline">\(\theta=\{ \theta,\phi, \alpha_{ng}, \forall{n}, g \}\)</span> and Eq.<a href="dl-model-for-scRNA-seq.html#eq:eq2">(3.2)</a> is solved by the stochastic gradient descent approach while a model is trained.</p>
<p>All the surveyed papers that deploy VAE follow this general modeling process. However, a more general formulation has a loss function defined as</p>
<p><span class="math display" id="eq:eq6">\[\begin{equation}
L(\Theta) = - L(\Theta) + \sum_{k=1}^{K}\lambda_{k}L_{k}(\Theta)\tag{3.6}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(L_{k}\forall{k}=1,…,K\)</span> are losses for different functions (clustering, cell type prediction, etc) and <span class="math inline">\(\lambda_{k}\)</span>s are the Lagrange multipliers. With this general formulation, for each paper, we examined the specific choices of data distribution <span class="math inline">\(p(x_{gn} \vert v_{gn},\alpha_{gn})\)</span> that defines <span class="math inline">\(L(\Theta)\)</span>, different <span class="math inline">\(L_{k}\)</span> designed for specific functions, and how the decoder and encoder were applied to model different aspects of scRNA-seq data.</p>

</div>
<div id="ae" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Autoencoders (AEs) for scRNA-seq data</h2>
<p>AEs learn the low dimensional latent representation <span class="math inline">\(\mathbf{z}_{n} \in \mathbb{R}^{d}\)</span> of expression <span class="math inline">\(x_{n}\)</span>. The AE includes an encoder <span class="math inline">\(E_{\phi}\)</span> and a decoder <span class="math inline">\(D_{\theta}\)</span> (Fig.<a href="dl-model-for-scRNA-seq.html#fig:Figure2">3.1</a>B) such that</p>
<p><span class="math display" id="eq:eq7">\[\begin{equation}
    z_{n}=E_{\phi}(x_{n}); \hat{x_{n}} = D_{\theta}(z_{n})  \tag{3.7}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\Theta = \{\theta,\phi\}\)</span> are encoder and decoder weight parameters and <span class="math inline">\(\hat{x_{n}}\)</span> defines the parameters (e.g. mean) of the likelihood <span class="math inline">\(p(x_{n} \vert \Theta)\)</span> (Fig.<a href="scRNA-seq-pipeline.html#fig:Figure1">2.1</a>B) and is often considered as imputed and denoised expressions. Additional design can be included in an AE model for batch correction, clustering, and other objectives.</p>
<p>The training of an AE model is generally carried out by stochastic gradient descent algorithms to minimize the loss similar to Eq.<a href="dl-model-for-scRNA-seq.html#eq:eq6">(3.6)</a> except <span class="math inline">\(L(\Theta) = -logp(x_{n}|\Theta)\)</span>. When <span class="math inline">\(p(x_{n}│Θ)\)</span> is the Gaussian, <span class="math inline">\(L(\Theta)\)</span> becomes the mean square error (MSE) loss</p>
<p><span class="math display" id="eq:eq8">\[\begin{equation}
    L(\Theta)=\sum_{n=1}^{N}||x_{n} - \hat{x}_{n}||_{2}^{2} \tag{3.8} 
\end{equation}\]</span></p>
<p>Because different AE models differ in their AE architectures and loss functions, we will discuss the specific architecture and loss functions for each reviewed DL model in Section <a href="DatasetEvalMetric.html#DatasetEvalMetric">4</a>.</p>

</div>
<div id="GANs" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Generative adversarial networks (GANs) for scRNA-seq data</h2>
<p>GANs have been used for imputation, data generation, and augmentation of the scRNA-seq analysis. Without loss of generality, the GAN, when applied to scRNA-seq, is designed to learn how to generate gene expression profiles from <span class="math inline">\(p_{x}\)</span>, the distribution of <span class="math inline">\(x_{n}\)</span>. The vanilla GAN consists of two deep neural networks [48]. The first network is the generator <span class="math inline">\(G_{\theta}(z_{n},y_{n})\)</span> with parameter <span class="math inline">\(\theta\)</span>, a noise vector <span class="math inline">\(z_{n}\)</span> from the distribution <span class="math inline">\(p_{z}\)</span> and a class label <span class="math inline">\(y\)</span> (e.g. cell type) and is trained to generate <span class="math inline">\(x_{f}\)</span>, a “fake” a gene expression (Fig.<a href="dl-model-for-scRNA-seq.html#fig:Figure2">3.1</a>C). The second network is the discriminator network <span class="math inline">\(D_{\phi_{D}}\)</span> with parameters <span class="math inline">\(\phi_{D}\)</span>, trained to distinguish between the “real” sample <span class="math inline">\(x\)</span> from <span class="math inline">\(x_{f}\)</span> (Fig.<a href="scRNA-seq-pipeline.html#fig:Figure1">2.1</a>C). Both networks, <span class="math inline">\(G_{\theta}\)</span> and <span class="math inline">\(D_{\phi_{D}}\)</span> are trained to outplay each other, resulting in a minimax game, in which <span class="math inline">\(G_{\theta}\)</span> is forced by <span class="math inline">\(D_{\phi_{D}}\)</span> to produce better samples, which, when converge, can fool the discriminator <span class="math inline">\(D_{\phi_{D}}\)</span>, thus becoming samples from <span class="math inline">\(p_{x}\)</span>. The vanilla GAN suffers heavily from training instability and mode collapsing <span class="citation">(<a href="Table.html#ref-RN104" role="doc-biblioref">Rosenberg and Hirschberg 2007</a>)</span><!--[44]-->. To that end, Wasserstein GAN (WGAN) was developed with the WGAN loss <span class="citation">(<a href="Table.html#ref-RN181" role="doc-biblioref">Rousseeuw 1987</a>)</span><!--[46]-->:</p>
<p><span class="math display" id="eq:eq10">\[\begin{equation}
  L(\Theta)=\max_{\phi_{D}}\sum_{n=1}^{N}D_{\phi_{D}}(x_{n})-\sum{n=1}^{N}D_{\phi_{D}}(G_{\theta}(z_{n},y_{n})) \tag{3.9}
\end{equation}\]</span></p>
<p>Additional terms can also be added to (<a href="dl-model-for-scRNA-seq.html#eq:eq10">(3.9)</a>) to constrain the functions of the generator. Training based on the WGAN loss in Eq. (<a href="dl-model-for-scRNA-seq.html#eq:eq10">(3.9)</a> ) amounts to a min-max optimization, which iterates between the discriminator and generator, where each optimization is achieved by a stochastic gradient descent algorithm through backpropagation. The WGAN requires D to be K-Lipschitz continuous <span class="citation">(<a href="Table.html#ref-RN104" role="doc-biblioref">Rosenberg and Hirschberg 2007</a>)</span><!--[44]-->, which can be satisfied by adding the gradient penalty to the WGAN loss <span class="citation">(<a href="Table.html#ref-RN103" role="doc-biblioref">Strehland and Ghosh 2002</a>)</span><!--[47]-->. Once the training is done, the generator <span class="math inline">\(G_{\phi_{G}}\)</span> can be used to generate gene expression profiles of new cells.</p>

</div>
<div id="supervised" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Supervised deep learning models</h2>
<p>Supervised deep learning models, including deep neural networks (DNN), convolutional neural network (CNN), and capsule networks (CapsNet), have been used for cell type identifications [51-53] and functional predictions [54]. The general supervised deep learning model <span class="math inline">\(F\)</span> takes <span class="math inline">\(x_{n}\)</span> as an input and outputs <span class="math inline">\(p(y_{n}|x_{n})\)</span>, the probability of phenotype label <span class="math inline">\(y_n\)</span> (e.g. a cell type) as</p>
<p><span class="math display" id="eq:eq10v2">\[\begin{equation}
    p(y_n│x_n)=F(x_n),  \tag{3.10}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(F\)</span> can be DNN, CNN, or CapsNet. We omit the discussion of DNN and CNN as they are widely used in different applications and there are many excellent surveys on them [55]. We will focus our discussion on CasNet next.</p>
<p>A CasNet takes an expression <span class="math inline">\(x_n\)</span> to first form a feature extraction network (consisting of <span class="math inline">\(L\)</span> parallel single-layer neural networks) followed by a classification capsule network. Each of the <span class="math inline">\(L\)</span> parallel feature extraction layers generates a primary capsule <span class="math inline">\(u_l∈\mathbb{R}^{d_p}\)</span> as</p>
<p><span class="math display" id="eq:11v2">\[\begin{equation}
    u_l=ReLU(W_{P,l},x_{n})  \forall{l}=1,…,L,  \tag{3.11}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(W_{P,l}∈\mathbb{R}^{d_p×G}\)</span> is the weight matrix. Then, the primary capsules are fed into the capsule network to compute <span class="math inline">\(K\)</span> label capsules <span class="math inline">\(v_k∈\mathbb{R}^{d_t}\)</span>, one for each label, as</p>
<p><span class="math display" id="eq:12v2">\[\begin{equation}
 v_k=squash(\sum_{l}^{L}c_{kl}W_{kl}u_{l}) \space ∀k=1,…,K, \tag{3.12}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(squash\)</span> is the squashing function [56] to normalize the magnitude of its input vector to be less than one, <span class="math inline">\(W_{kl}\)</span> is another trainable weight matrix, and <span class="math inline">\(c_{kl} \space \forall{l}=1,…,L\)</span>, are the coupling coefficients that represent the probability distribution of each primary capsule’s impact on the predicted label <span class="math inline">\(k\)</span>. Parameters <span class="math inline">\(c_{kl}\)</span> are not trained but computed through the dynamic routing process proposed in the original capsule networks [52]. The magnitude of each capsule <span class="math inline">\(v_k\)</span> represents the probability of predicting label <span class="math inline">\(k\)</span> for input <span class="math inline">\(x_n\)</span>. Once trained, the important primary capsules for each label and then the most significant genes for each important primary capsule can be used to interpret biological functions associated with the prediction.</p>
<p>The training of the supervised models for classification overwhelmingly minimizes the cross-entropy loss by stochastic gradient descent computed by a back-propagation algorithm.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="scRNA-seq-pipeline.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="DatasetEvalMetric.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Huang-AI4Medicine-Lab/survey-of-DL-for-scRNA-seq-analysis/edit/main/Manuscript/overview-of-common-dl-models-for-scRNA-seq-analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["survey-of-DL-for-scRNA-seq-analysis.pdf", "survey-of-DL-for-scRNA-seq-analysis.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
