<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<style>body{background-color:white;}</style>
<script src="lib/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="lib/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="lib/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="lib/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="lib/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="lib/kePrint-0.0.1/kePrint.js"></script>
<link href="lib/lightable-0.0.1/lightable.css" rel="stylesheet" />

</head>
<body>
<div style="border: 1px solid #ddd; padding: 5px; overflow-x: scroll; width:100%; "><table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>Evaluation metrics used in surveyed DL algorithms</caption>
 <thead>
  <tr>
   <th style="text-align:left;"> EvaluationMethod </th>
   <th style="text-align:left;"> Equations </th>
   <th style="text-align:left;"> Explanation </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;width: 1em; "> Pseudobulk RNA-seq </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Average of normalized (log2-transformed) scRNA-seq counts across cells is calculated and then correlation coefficient between the pseudobulk and the actual bulk RNA-seq profile of the same cell type is evaluated. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Mean squared error (MSE) </td>
   <td style="text-align:left;"> $MSE=\frac{1}{n} \sum_{i=1}^{n}(x_{i}- \hat{x}_{i})^{2}$ </td>
   <td style="text-align:left;"> MSE assesses the quality of a predictor, or an estimator, from a collection of observed data $x$, with $\hat{x}$ being the predicted values. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Pearson correlation </td>
   <td style="text-align:left;"> $\rho_{X,Y}=\frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}}$ </td>
   <td style="text-align:left;"> where cov() is the covariance, $\sigma X$ and $\sigma Y$ are the standard deviation of $X$ and $Y$, respectively. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Spearman correlation </td>
   <td style="text-align:left;"> $\rho_{s}=\rho_{r_{X},r_{Y}}=\frac{cov(r_X,r_Y)}{\sigma_{r_X}\sigma_{r_Y}}$ </td>
   <td style="text-align:left;"> The Spearman correlation coefficient is defined as the Pearson correlation coefficient between the rank variables, where $r_{X}$ is the rank of $X$. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Entropy of accuracy, Hacc [21] </td>
   <td style="text-align:left;"> $H_{acc}=-\frac{1}{M} \sum_{i=1}^{M} \sum_{j=1}^{N_i} p_i(x_j)logp_{i}(x_{j})$ </td>
   <td style="text-align:left;"> Measures the diversity of the ground-truth labels within each predicted cluster group. $p_{i}(x_{j})$ (or $q_{i}(x_{j})$) are the proportions of cells in the $j$th ground-truth cluster (or predicted cluster) relative to the total number of cells in the $i$th predicted cluster (or ground-truth clusters), respectively. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Entropy of purity, Hpur [21] </td>
   <td style="text-align:left;"> $H_{pur}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M_i}q_i(x_j)logq_{i}(x_{j})$ </td>
   <td style="text-align:left;"> Measures the diversity of the predicted cluster labels within each ground-truth group </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Entropy of mixing [32] </td>
   <td style="text-align:left;"> $E=\sum_{i=1}^{C}p_{i}\log(p_{i})$ </td>
   <td style="text-align:left;"> This metric evaluates the mixing of cells from different batches in the neighborhood of each cell. $C$ is the number of batches, and $p_{i}$ is the proportion of cells from batch $i$ among $N$ nearest cells. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Mutual Information (MI) [164] </td>
   <td style="text-align:left;"> $MI(U,V)=\sum_{i}^{|U|}\sum_{j=1}^{|V|}P_{UV}(i,j)log(\frac{P_{UV}(i,j)}{P_{U}(i)P_{V}(j)})$ </td>
   <td style="text-align:left;"> where $P_{U}(i)=\frac{|U_{i}|}{N}$ and $P_{V}(j)=\frac{|V_{j}|}{N}$. Also, define the joint distribution probability is $P_{UV}(i,j)=\frac{|U_{i} \cap V_{j}|}{N}$. The $MI$ is a measure of mutual dependency between two cluster assignments $U$ and $V$. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Normalized Mutual Information (NMI) [165] </td>
   <td style="text-align:left;"> $NMI(U,V)=\frac{2 \times MI(U,V)}{[H(U)+H(V)]}$ </td>
   <td style="text-align:left;"> where $H(U)=\sum P_{U}(i)log(P_{U}(i)), H(V)=\sum P_{V}(i)log(P_V(i))$. The $NMI$ is a normalization of the $MI$ score between 0 and 1. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Kullback–Leibler (KL) divergence [166] </td>
   <td style="text-align:left;"> $D_{KL}(P||Q)=\sum_{x \in \chi}P(x)log(\frac{P(x)}{Q(x)})$ </td>
   <td style="text-align:left;"> where discrete probability distributions $P$ and $Q$  are defined on the same probability space $&lt;U+03C7&gt;$. This relative entropy is the measure for directed divergence between two distributions. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Jaccard Index </td>
   <td style="text-align:left;"> $J(U,V)=\frac{\lfloor U \cap V \rfloor}{\lfloor U \cup V \rfloor}$ </td>
   <td style="text-align:left;"> $0 = J(U,V) = 1$. $J = 1$ if clusters $U$ and $V$ are the same. If $U$ are $V$ are empty, $J$ is defined as 1. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Fowlkes-Mallows Index for two clustering algorithms (FM) </td>
   <td style="text-align:left;"> $FM=\sqrt{ \frac{TP}{TP + FP} \times \frac{TP}{TP+FN} }$ </td>
   <td style="text-align:left;"> TP as the number of pairs of points that are present in the same cluster in both $U$ and $V$; $FP$ as the number of pairs of points that are present in the same cluster in $U$ but not in $V$; $FN$ as the number of pairs of points that are present in the same cluster in $V$ but not in $U$; and TN as the number of pairs of points that are in different clusters in both U and V. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Rand index (RI) </td>
   <td style="text-align:left;"> $RI=\frac{(a+b)}{\binom{n}{2}}$ </td>
   <td style="text-align:left;"> Measure of constancy between two clustering outcomes, where $a$ (or $b$) is the count of pairs of cells in one cluster (or different clusters) from one clustering algorithm but also fall in the same cluster (or different clusters) from the other clustering algorithm. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Adjusted Rand index (ARI) [167] </td>
   <td style="text-align:left;"> $ARI=\frac{RI-E[RI]}{max(RI)-E[RI]}$ </td>
   <td style="text-align:left;"> ARI is a corrected-for-chance version of RI, where $E[RI]$ is the expected Rand Index. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Silhouette index </td>
   <td style="text-align:left;"> $s(i)=\frac{b(i)-a(i)}{max(a(i),b(i))}$ </td>
   <td style="text-align:left;"> where $a(i)$ is the average dissimilarity of ith cell to all other cells in the same cluster, and $b(i)$ is the average dissimilarity of ith cell to all cells in the closest cluster. The range of $s(i)$ is [-1,1], with 1 to be well-clustered and -1 to be completely misclassified. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Maximum Mean Discrepancy (MMD) [65] </td>
   <td style="text-align:left;"> $MMD(F,p,q)=sup_{f \in F}||\mu_{p}-\mu_{q}||_{f}$ </td>
   <td style="text-align:left;"> $MMD$ is a non-parametric distance between distributions based on the reproducing kernel Hilbert space, or, a distance-based measure between two distribution $p$ and $q$ based on the mean embeddings $\mu_{p}$ and $\mu_{q}$ in a reproducing kernel Hilbert space $F$. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> k-Nearest neighbor batch-effect test (kBET) [168] </td>
   <td style="text-align:left;"> $a_{n}^{k}=\sum_{l=1}^{L}\frac{(N_{nl}^{k} - k \bullet f_{l})^{2}}{k \bullet f_{l}} ~ X_{L-1}^{2}$ </td>
   <td style="text-align:left;"> Given a dataset of $N$ cells from $L$ batches with $N_l$ denoting the number of cells in batch $l$, $N_{nl}^{k}$ is the number of cells from batch $l$ in the $k$-nearest neighbors of cell $n$, $f_{l}$ is the global fraction of cells in batch $l$, or $f_{l}=\frac{N_l}{N}$, and  $X_{L-1}^{2}$ denotes the $X^{2}$ distribution with $L-1$ degrees of freedom. It uses a $X^{2}$-based test for random neighborhoods of fixed size to determine the significance (“well-mixed”). </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Local Inverse Simpson’s Index (LISI) [34] </td>
   <td style="text-align:left;"> $\frac{1}{ \lambda(n)}=\frac{1}{\sum_{l=1}^{L}(p(l))^{2}}$ </td>
   <td style="text-align:left;"> This is the inverse Simpson’s Index in the $k$-nearest neighbors of cell $n$ for all batches, where $p(l)$ denotes the proportion of batch $l$ in the $k$-nearest neighbors. The score reports the effective number of batches in the $k$-nearest neighbors of cell $n$. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Homogeneity </td>
   <td style="text-align:left;"> $HS=1-\frac{H(P(U|V))}{H(P(U))}$ </td>
   <td style="text-align:left;"> where $H()$ is the entropy, and $U$ is the ground-truth assignment and $V$ is the predicted assignment. The $HS$ range from 0 to 1, where 1 indicates perfectly homogeneous labeling. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Completeness </td>
   <td style="text-align:left;"> $CS=1-\frac{H(P(V|U))}{H(P(V))}$ </td>
   <td style="text-align:left;"> Its values range from 0 to 1, where 1 indicates all members from a ground-truth label are assigned to a single cluster. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> V-Measure [169] </td>
   <td style="text-align:left;"> $V_{\beta}=\frac{(1+\beta)HS \times CS}{\beta HS + CS}$ </td>
   <td style="text-align:left;"> where $\beta$ indicates the weight of $HS$. $V$-Measure is symmetric, i.e. switching the true and predicted cluster labels does not change $V$-Measure. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Precision, recall </td>
   <td style="text-align:left;"> $Precision = \frac{TP}{TP+FP}, recall=\frac{TP}{TP+FN}$ </td>
   <td style="text-align:left;"> TP: true positive, FP: false positive, FN, false negative. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> Accuracy </td>
   <td style="text-align:left;"> $Accuracy = \frac{TP+TN}{N}$ </td>
   <td style="text-align:left;"> N: all samples tested, TN: true negative </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> F1-score </td>
   <td style="text-align:left;"> $F_{1}=\frac{2Precision \bullet Recall}{Precision+Recall}$ </td>
   <td style="text-align:left;"> A harmonic mean of precision and recall. It can be extended to $F_\beta$ where $\beta$ is a weight between precision and recall (similar to $V$-measure). </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 1em; "> AUC, RUROC </td>
   <td style="text-align:left;"> ![curve](D:/Github/survey-of-DL-for-scRNA-seq-analysis/Figures/Table3_AUC_Figure.png) </td>
   <td style="text-align:left;"> Area Under Curve (grey area). Receiver operating characteristic (ROC) curve (red line). A similar measure can be performed on the Precision-Recall curve (PRC), or AUPRC. Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model (mostly for an imbalanced dataset). </td>
  </tr>
</tbody>
</table></div> <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"]]}})</script><script async src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
