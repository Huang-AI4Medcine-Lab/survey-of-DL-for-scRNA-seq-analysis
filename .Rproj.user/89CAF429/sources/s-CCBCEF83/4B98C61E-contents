
## Autoencoders (AEs) for scRNA-seq data {#ae}


AEs learn the low dimensional latent representation $\mathbf{z}_{n} \in \mathbb{R}^{d}$ of expression $x_{n}$. The AE includes an encoder $E_{\phi}$ and a decoder $D_{\theta}$ (Fig.\@ref(fig:Figure2)B) such that

\begin{equation}
	z_{n}=E_{\phi}(x_{n}); \hat{x_{n}} = D_{\theta}(z_{n})	(\#eq:eq7)
\end{equation}

where $\Theta = \{\theta,\phi\}$ are encoder and decoder weight parameters and $\hat{x_{n}}$ defines the parameters (e.g. mean) of the likelihood $p(x_{n} \vert \Theta)$ (Fig.\@ref(fig:Figure1)B) and is often considered as imputed and denoised expressions. Additional design can be included in an AE model for batch correction, clustering, and other objectives.

The training of an AE model is generally carried out by stochastic gradient descent algorithms to minimize the loss similar to Eq.\@ref(eq:eq6) except $L(\Theta) = -logp(x_{n}|\Theta)$. When $p(x_{n}│Θ)$ is the Gaussian,  $L(\Theta)$ becomes the mean square error (MSE) loss   

\begin{equation}
	L(\Theta)=\sum_{n=1}^{N}||x_{n} - \hat{x}_{n}||_{2}^{2} (\#eq:eq8) 
\end{equation}

Because different AE models differ in their AE architectures and loss functions, we will discuss the specific architecture and loss functions for each reviewed DL model in Section \@ref(DatasetEvalMetric).   
